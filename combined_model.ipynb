{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-world Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import scipy.stats as stats\n",
    "\n",
    "import seaborn as sns\n",
    "import json\n",
    "import openai\n",
    "import random\n",
    "import os\n",
    "import collections\n",
    "import itertools\n",
    "import copy\n",
    "import dataloader\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "import netgraph\n",
    "import scipy\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import ast\n",
    "from utils import get_response, summarize_reasons\n",
    "import link_prediction\n",
    "import dcm\n",
    "\n",
    "rename_models = {\n",
    "    'gpt-3.5-turbo' : 'GPT-3.5',\n",
    "    'gpt-4o-mini' : 'GPT-4 Mini',\n",
    "    'meta-meta-llama-3-70b-instruct' : 'LLAMA-3',\n",
    "    'meta/meta-llama-3-70b-instruct' : 'LLAMA-3',\n",
    "    'claude-3-5-sonnet-20240620' : 'Claude 3.5',  \n",
    "    'gpt-3.5-turbo+link_prediction' : 'GPT-3.5',\n",
    "    'gpt-4o-mini+link_prediction' : 'GPT-4 Mini',\n",
    "    'meta-meta-llama-3-70b-instruct+link_prediction' : 'LLAMA-3',\n",
    "    'claude-3-5-sonnet-20240620+link_prediction' : 'Claude 3.5',\n",
    "}\n",
    "\n",
    "def network_growth(G0, temperature, name, num_choices=1, method='llm', num_samples=-1, num_nodes_samples=-1, model='gpt-4-1106-preview', sampling_strategy='random'):\n",
    "    # Set seed\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)    \n",
    "\n",
    "    # Copy the ground truth graph\n",
    "    G = G0.copy()\n",
    "   \n",
    "    Gs = [G.copy()]\n",
    "\n",
    "    profiles = nx.get_node_attributes(G, 'features')\n",
    "\n",
    "    if sampling_strategy == 'link_prediction':\n",
    "        sk_model = link_prediction.train_link_predictor(G, profiles=profiles, name=name)\n",
    "    else:\n",
    "        sk_model = None\n",
    "\n",
    "    # Edges to drop\n",
    "    dropped_edges = []\n",
    "\n",
    "    if num_nodes_samples > 0 and num_nodes_samples < len(G):\n",
    "        print(f'Sampling {num_nodes_samples} nodes from {len(G)} nodes')\n",
    "        print(G.nodes())\n",
    "\n",
    "        nodes = random.sample(list(G.nodes()), num_nodes_samples)\n",
    "    else:\n",
    "        nodes = G.nodes()\n",
    "\n",
    "\n",
    "    # Drop one neighbor for each node\n",
    "    for v in nodes:\n",
    "        dropped_v_edges = []\n",
    "        for _ in range(num_choices):\n",
    "            if len(list(G.neighbors(v))) > 0:\n",
    "                \n",
    "                while True:\n",
    "                    u = random.choice(list(G.neighbors(v)))\n",
    "                    if (v, u) not in dropped_edges:\n",
    "                        dropped_v_edges.append((v, u))\n",
    "                        G.remove_edge(v, u)\n",
    "                        break\n",
    "\n",
    "        dropped_edges.append(dropped_v_edges)\n",
    "\n",
    "    Gs = [G.copy()]\n",
    "    results = []\n",
    "    candidates = []\n",
    "\n",
    "\n",
    "    for i, t in enumerate(nodes):\n",
    "\n",
    "        if method == 'llm':\n",
    "            print('{}/{}'.format(i + 1, len(nodes)))\n",
    "            result, candidate = select_neighbor(G, t, profiles, temperature, num_choices=len(dropped_edges[i]), dropped_nodes=[u for (_, u) in dropped_edges[i]], num_samples=num_samples, model=model, sampling_strategy=sampling_strategy, sk_model=sk_model)\n",
    "\n",
    "            if result:\n",
    "                for r in result:\n",
    "                    v = r['name']\n",
    "                    r['edge'] = (t, v)\n",
    "                    G.add_edge(t, v, similarity=r['similarity'])\n",
    "                results.append(result)\n",
    "\n",
    "            candidates.append(candidate)\n",
    "        if method == 'ground_truth':\n",
    "            if num_samples > num_choices:\n",
    "                choice_set = random.sample([v for v in G.nodes() if v != t], num_samples - num_choices)\n",
    "            else:\n",
    "                choice_set = [v for v in G.nodes() if v != t]\n",
    "            \n",
    "            new_nodes = [e[1] for e in dropped_edges[i]]\n",
    "\n",
    "            choice_set = choice_set + new_nodes\n",
    "\n",
    "\n",
    "            result = []\n",
    "\n",
    "            for v in new_nodes:\n",
    "\n",
    "                profiles[t]['neighbors'] = list(G.neighbors(t))\n",
    "                profiles[v]['neighbors'] = list(G.neighbors(v))\n",
    "                profiles[t]['degree'] = len(profiles[t]['neighbors'])\n",
    "                profiles[v]['degree'] = len(profiles[v]['neighbors'])\n",
    "\n",
    "                similarity = measure_similarity(profiles[t], profiles[v])\n",
    "                G.add_edge(t, v, similarity=similarity, weight=similarity['common_attributes'])\n",
    "            \n",
    "                result.append({'name' : v, 'similarity' : similarity, 'reason' : method, 'dropped' : True})\n",
    "\n",
    "            candidate = []\n",
    "\n",
    "            for v in choice_set:\n",
    "                profiles[t]['neighbors'] = list(G.neighbors(t))\n",
    "                profiles[v]['neighbors'] = list(G.neighbors(v))\n",
    "                profiles[t]['degree'] = len(profiles[t]['neighbors'])\n",
    "                profiles[v]['degree'] = len(profiles[v]['neighbors'])\n",
    "\n",
    "                similarity = measure_similarity(profiles[t], profiles[v])\n",
    "                candidate.append({'name' : v, 'similarity' : similarity, 'reason' : method})\n",
    "\n",
    "            candidates.append(candidate)\n",
    "            results.append(result)\n",
    "\n",
    "            print(f'Node: {t}, Links: {result}, Candidates: {candidate}')\n",
    "\n",
    "        Gs.append(G.copy())\n",
    "\n",
    "    return Gs, results, candidates\n",
    "\n",
    "def fit_dcm(results):\n",
    "\n",
    "    similarities = [r['similarity'] for result in results for r in result]\n",
    "    similarities_df = pd.DataFrame.from_records(similarities)\n",
    "    similarities_df = sm.add_constant(similarities_df)\n",
    "\n",
    "    outcomes = np.array([r['edge'][1] for result in results for r in result])\n",
    "\n",
    "    print(similarities_df)\n",
    "\n",
    "    mnl_model = sm.MNLogit(outcomes, similarities_df)\n",
    "    mnl_results = mnl_model.fit()\n",
    "\n",
    "    print(mnl_results.summary())\n",
    "\n",
    "    return mnl_results\n",
    "\n",
    "def measure_similarity(profile1, profile2):\n",
    "\n",
    "    similarity = {\n",
    "        'common_attributes' : 0,\n",
    "        'common_neighbors' : len(set(profile1['neighbors']) & set(profile2['neighbors'])),\n",
    "        'degree' : profile2['degree'],\n",
    "    }\n",
    "\n",
    "    for k in profile1.keys():\n",
    "        if k != 'name' and k != 'neighbors' and k in profile2.keys():\n",
    "            if isinstance(profile1[k], list):\n",
    "                similarity['common_attributes'] += len(set(profile1[k]) & set(profile2[k]))\n",
    "            elif profile1[k] == profile2[k]:\n",
    "                similarity['common_attributes'] += 1\n",
    "        \n",
    "    return similarity\n",
    "\n",
    "\n",
    "def select_neighbor(G, t, profiles, temperature, num_choices=1, num_samples=-1, dropped_nodes=[], model='gpt-4-1106-preview', sampling_strategy='random', sk_model=None):\n",
    "\n",
    "    if num_samples > 0:\n",
    "        if sampling_strategy == 'random':\n",
    "            choice_set = random.sample([v for v in G.nodes() if v != t and v not in G.neighbors(t)], max(0, num_samples - len(dropped_nodes))) + dropped_nodes\n",
    "        elif sampling_strategy == 'pagerank':\n",
    "            pagerank_scores = nx.pagerank(G)\n",
    "            temp_nodes = [v for v in G.nodes() if v != t and v not in G.neighbors(t)]\n",
    "            # pagerank scores to numpy\n",
    "            pagerank_scores = np.array([pagerank_scores[v] for v in temp_nodes])\n",
    "            choice_set = np.random.choice(temp_nodes, size=min(num_samples - len(dropped_nodes), len(temp_nodes)), replace=False, p=pagerank_scores/np.sum(pagerank_scores)).tolist() + dropped_nodes\n",
    "        elif sampling_strategy == 'degree':\n",
    "            temp_nodes = [v for v in G.nodes() if v != t and v not in G.neighbors(t)]\n",
    "            degree_scores = np.array([G.degree(v) for v in temp_nodes])\n",
    "            choice_set = np.random.choice(temp_nodes, size=min(num_samples - len(dropped_nodes), len(temp_nodes)), replace=False, p=degree_scores/np.sum(degree_scores)).tolist() + dropped_nodes\n",
    "        elif sampling_strategy == 'link_prediction':\n",
    "            choice_set = link_prediction.recommend_friends(sk_model, G, profiles, t, k=num_samples - len(dropped_nodes)) + dropped_nodes\n",
    "    else:\n",
    "        choice_set = [v for v in G.nodes() if v != t and v not in G.neighbors(t)]\n",
    "\n",
    "    candidate_profiles = []\n",
    "\n",
    "    for v in choice_set + [t]:\n",
    "        profiles[v]['neighbors'] = list(G.neighbors(v))\n",
    "        profiles[v]['degree'] = len(profiles[v]['neighbors']) \n",
    "        profiles[v]['name'] = v                 \n",
    "        candidate_profiles.append(profiles[v])\n",
    "\n",
    "    random.shuffle(candidate_profiles)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    # Task\n",
    "    Your task is to select a set of people to be friends with.\n",
    "\n",
    "    # Profile\n",
    "    Your profile is given below after chevrons:\n",
    "    <PROFILE>\n",
    "    {json.dumps(profiles[t])}\n",
    "    </PROFILE>\n",
    "\n",
    "    # Candidate Profiles\n",
    "    The cadidate profiles to be friends with are given below after chevrons:\n",
    "\n",
    "    <PROFILES>\n",
    "    {json.dumps(candidate_profiles)}\n",
    "    </PROFILES>\n",
    "\n",
    "    # Output\n",
    "    The output should be given a list of JSON objects with the following structure\n",
    "\n",
    "    [\n",
    "        {{\n",
    "            \"name\" : name of the person you selected,\n",
    "            \"reason\" : reason for selecting the person\n",
    "        }}, ...\n",
    "    ]\n",
    "\n",
    "    # Notes\n",
    "    * The output must be a list of JSON objects ranked in the order of preference.\n",
    "    * You can make at most {num_choices} selection{'s' if num_choices > 1 else ''}.\n",
    "    * Your output must be contained within the json markdown cue.\n",
    "    \n",
    "    ```json\n",
    "    \"\"\"   \n",
    "    \n",
    "    for _ in range(1):\n",
    "        try:\n",
    "            ans = get_response(prompt, temperature=temperature, model=model)\n",
    "            try:\n",
    "                results = json.loads(ans.split('```')[0])\n",
    "            except:\n",
    "                results = json.loads(ans.split('```json')[1].split('```')[0])\n",
    "\n",
    "            filtered_results = []\n",
    "            for result in results:\n",
    "                v = result['name']\n",
    "                if v in G.nodes():\n",
    "                    result['similarity'] = measure_similarity(profiles[t], profiles[v])\n",
    "                    filtered_results.append(result)\n",
    "\n",
    "                    result['dropped'] = v in dropped_nodes\n",
    "\n",
    "            print(f'Node: {t}, Links: {filtered_results}')\n",
    "\n",
    "            candidates = []\n",
    "\n",
    "            for candidate_profile in candidate_profiles:\n",
    "                similarity = measure_similarity(profiles[t], candidate_profile)\n",
    "                candidates.append({'name' : candidate_profile['name'], 'similarity' : similarity})\n",
    "\n",
    "            return filtered_results, candidates\n",
    "        except Exception as e:\n",
    "            print('error', e)\n",
    "\n",
    "    return [], []\n",
    "\n",
    "\n",
    "def run_network_formation_experiment(name, num_simulations, outfile, temperatures, method, num_choices, num_samples, num_nodes_samples, model, dataloader_fn, sampling_strategy='random'):\n",
    "    networks = dataloader_fn()\n",
    "    \n",
    "    saved_scenarios = set()\n",
    "\n",
    "    if os.path.exists(outfile):\n",
    "        with open(outfile) as f:\n",
    "            lines = f.read().splitlines()\n",
    "\n",
    "            for line in lines:\n",
    "                scenario = json.loads(line)\n",
    "                saved_scenarios.add((scenario['name'], scenario['ego'], scenario['simulation'], scenario['temperature'], scenario['num_samples'], scenario['num_choices']))\n",
    "\n",
    "\n",
    "    f = open(outfile, 'a+')\n",
    "\n",
    "\n",
    "    for ego, G0 in networks.items():\n",
    "        for i in range(num_simulations):\n",
    "            for temperature in temperatures:\n",
    "                if (name, ego, i, temperature, num_samples, num_choices) in saved_scenarios:\n",
    "                    print(f'Skipping simulation for name={name}, ego={ego}, i={i}, temperature={temperature}, num_choices={num_choices}, num_samples={num_samples}, method={method}')\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f'Running simulation for name={name}, ego={ego}, i={i}, temperature={temperature}, num_choices={num_choices}, num_samples={num_samples}, method={method}')\n",
    "\n",
    "                    Gs, results, candidates = network_growth(G0, temperature=temperature, method=method, name=name, num_choices=num_choices, num_samples=num_samples, num_nodes_samples=num_nodes_samples, model=model, sampling_strategy=sampling_strategy)\n",
    "\n",
    "                    temp = {\n",
    "                        'name' : name,\n",
    "                        'ego' : ego,\n",
    "                        'temperature' : temperature,\n",
    "                        'simulation' : i,\n",
    "                        'num_choices' : num_choices,\n",
    "                        'num_samples' : num_samples,\n",
    "                        'graphs' : [nx.to_dict_of_dicts(G) for G in [Gs[0], Gs[-1]]],\n",
    "                        'results' : results,\n",
    "                        'candidates' : candidates,\n",
    "                        'model' : model,\n",
    "                        'sampling_strategy' : sampling_strategy\n",
    "                    }\n",
    "\n",
    "                    f.write(json.dumps(temp) + '\\n')            \n",
    "\n",
    "                if method != 'llm':\n",
    "                    break\n",
    "\n",
    "    f.close()\n",
    "\n",
    "def draw_graph(G, ax, communities=None, palette=None):\n",
    "\n",
    "    pos = nx.spring_layout(G)\n",
    "\n",
    "    netgraph.Graph(G, node_layout=pos, node_color='#d35400', node_size=2.5, edge_color='#34495e', edge_width=1, ax=ax)\n",
    "\n",
    "    ax.set_axis_off()\n",
    "\n",
    "\n",
    "def generate_regression_table(filename, outfile, bias=True, log_transform=True, exclude_log=[]):\n",
    "\n",
    "    palette = ['#d35400', '#34495e', '#2980b9', '#e67e22', '#f1c40f', '#7f8c8d', '#27ae60', '#16a085', '#bdc3c7', '#1abc9c', '#2ecc71', '#3498db', '#9b59b6', '#8e44ad', '#ecf0f1']\n",
    "\n",
    "    with open(filename) as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for line in lines:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "    feature_names = ['degree', 'common_attributes', 'common_neighbors']\n",
    "\n",
    "    regression_table_df = []\n",
    "    \n",
    "    names = set([d['name'] for d in data])\n",
    "\n",
    "    for d in data:\n",
    "\n",
    "        log_likelihoods = {}\n",
    "\n",
    "        for num_features in range(len(feature_names) + 1):\n",
    "            for feature_combination in itertools.combinations(feature_names, num_features):\n",
    "                feature_combination = list(feature_combination)\n",
    "                theta, standard_errors, log_likelihood, _, probabilities, ame, sdame = dcm.fit_discrete_choice_model((d['results'], d['candidates']), feature_names=feature_combination, bias=bias, log_transform=log_transform, exclude_log=exclude_log, calculate_p_values=True, calculate_average_marginal_effects=True, input_type='results_candidates')\n",
    "\n",
    "\n",
    "                temp = {\n",
    "                    'Name' : d[\"name\"],\n",
    "                    'Ego' : d[\"ego\"],\n",
    "                    'Temperature' : d[\"temperature\"],\n",
    "                    'Simulation' : d[\"simulation\"],\n",
    "                    'Number of Choices' : d[\"num_choices\"],\n",
    "                    'Number of Samples' : d[\"num_samples\"],\n",
    "                    'Independent Variable' : feature_combination,\n",
    "                    'Coefficients' : theta[:-1].tolist(),\n",
    "                    'Standard Errors' : standard_errors[:-1].tolist(),\n",
    "                    'Log Likelihood' : log_likelihood,\n",
    "                    'Probabilities' : probabilities.tolist() if probabilities is not None else None,\n",
    "                    'AME' : ame.tolist() if ame is not None else None,\n",
    "                    'SE AME' : sdame.tolist() if sdame is not None else None,\n",
    "                }\n",
    "\n",
    "                log_likelihoods[tuple(sorted(feature_combination))] = log_likelihood\n",
    "                p_values = np.array([1 - stats.chi2.cdf(2 * (log_likelihood - log_likelihoods[tuple(sorted(feature_combination[:i] + feature_combination[i + 1:]))]), 1) for i in range(len(feature_combination))])\n",
    "\n",
    "                print(f'features: {feature_combination}, theta: {theta}, standard_errors: {standard_errors}, p-values: {p_values}, log_likelihood: {log_likelihood}, AME: {ame}, AME (SE): {sdame}')\n",
    "\n",
    "                temp['P-values'] = p_values.tolist()\n",
    "\n",
    "                \n",
    "\n",
    "                regression_table_df.append(temp)\n",
    "             \n",
    "    regression_table_df = pd.DataFrame.from_records(regression_table_df)\n",
    "\n",
    "    regression_table_df.to_excel(outfile)\n",
    "\n",
    "\n",
    "def compare_models(filenames1, filenames2, bias=True, log_transform=True, exclude_log=[], heatmap=True, suptitle='', supxlabel='', supylabel='', outfile='figures/comparison_between_models.png'):\n",
    "\n",
    "    palette = ['#d35400', '#34495e', '#2980b9', '#e67e22', '#f1c40f', '#7f8c8d', '#27ae60', '#16a085', '#bdc3c7', '#1abc9c', '#2ecc71', '#3498db', '#9b59b6', '#8e44ad', '#ecf0f1']\n",
    "\n",
    "    feature_names = ['degree', 'common_attributes', 'common_neighbors']\n",
    "\n",
    "    records_between_models = []\n",
    "    records_effects = [] \n",
    "\n",
    "    for filename1, filename2 in zip(filenames1, filenames2):\n",
    "\n",
    "        basename1 = filename1.split('+')\n",
    "        basename2 = filename2.split('+')\n",
    "\n",
    "        if len(basename1) == 3:\n",
    "            model1 = basename1[-2] + '+' + basename1[-1]\n",
    "        elif len(basename1) == 2:\n",
    "            model1 = basename1[-1]\n",
    "\n",
    "        if len(basename2) == 3:\n",
    "            model2 = basename2[-2] + '+' + basename2[-1]\n",
    "        elif len(basename2) == 2:\n",
    "            model2 = basename2[-1]\n",
    "\n",
    "        # remove file extension from model1\n",
    "        model1 = model1.replace('.jsonl', '')\n",
    "        model2 = model2.replace('.jsonl', '')\n",
    "\n",
    "\n",
    "        with open(filename1) as f:\n",
    "            lines1 = f.read().splitlines()\n",
    "\n",
    "        with open(filename2) as f:\n",
    "            lines2 = f.read().splitlines()\n",
    "\n",
    "        data1 = []\n",
    "\n",
    "        for line in lines1:\n",
    "            data1.append(json.loads(line))\n",
    "\n",
    "        data2 = []\n",
    "\n",
    "        for line in lines2:\n",
    "            data2.append(json.loads(line))\n",
    "\n",
    "\n",
    "        for d1, d2 in zip(data1, data2):\n",
    "            if d1['name'] != d2['name']:\n",
    "                print(f'Skipping {d1[\"name\"]} and {d2[\"name\"]} as they are not the same scenario')\n",
    "                continue\n",
    "            else:\n",
    "                print(f'Comparing {model1} and {model2} on {d1[\"name\"]} at temperature {d1[\"temperature\"]}')\n",
    "\n",
    "            distance_mean, distance_std, theta_spearman, theta1, theta2, sd1, sd2, ame1, ame2, sdame1, sdame2, p_values_ame1, p_values_ame2 = dcm.compare_models((d1['results'], d1['candidates']), (d2['results'], d2['candidates']), on='Alternative Set', method='tv', bias=bias, feature_names=feature_names, log_transform=log_transform, exclude_log=exclude_log, calculate_p_values=True, calculate_average_marginal_effects=True, input_type='results_candidates')\n",
    "\n",
    "            records_between_models.append({\n",
    "                'Name' : d1['name'].capitalize(),\n",
    "                'Model1': rename_models.get(model1, model1),\n",
    "                'Model2': rename_models.get(model2, model2),\n",
    "                'TV Distance': distance_mean,\n",
    "                'TV Distance Std': distance_std,\n",
    "                'Effect Spearman Correlation': theta_spearman,\n",
    "                'Theta1': theta1,  \n",
    "                'Theta2': theta2,\n",
    "                'StandardError1': sd1,\n",
    "                'StandardError2': sd2,\n",
    "                'AME1' : ame1,\n",
    "                'AME2' : ame2,\n",
    "                'StandardErrorAME1': sdame1,\n",
    "                'StandardErrorAME2': sdame2,\n",
    "                'P-values AME1': p_values_ame1,\n",
    "                'P-values AME2': p_values_ame2\n",
    "            })\n",
    "\n",
    "            for j, feature in enumerate(feature_names):\n",
    "\n",
    "                stars_1 = '***' if p_values_ame1[j] < 0.001 else '**' if p_values_ame1[j] < 0.01 else '*' if p_values_ame1[j] < 0.05 else ''\n",
    "                stars_2 = '***' if p_values_ame2[j] < 0.001 else '**' if p_values_ame2[j] < 0.01 else '*' if p_values_ame2[j] < 0.05 else ''\n",
    "\n",
    "                ame1_formatted = f\"{ame1[j]:.2f}{stars_1} ({sdame1[j]:.2f})\"\n",
    "                ame2_formatted = f\"{ame2[j]:.2f}{stars_2} ({sdame2[j]:.2f})\"\n",
    "\n",
    "                records_effects.append({\n",
    "                    'Name' : d1['name'].capitalize(),\n",
    "                    'Model': rename_models.get(model1, model1),\n",
    "                    'Label' : supxlabel,\n",
    "                    'Feature': feature,\n",
    "                    'AME': ame1_formatted,\n",
    "                })\n",
    "\n",
    "                records_effects.append({\n",
    "                    'Name' : d2['name'].capitalize(),\n",
    "                    'Model': rename_models.get(model2, model2),\n",
    "                    'Label' : supylabel,\n",
    "                    'Feature': feature,\n",
    "                    'AME': ame2_formatted\n",
    "                })\n",
    "\n",
    "    records_between_models_df = pd.DataFrame.from_records(records_between_models)\n",
    "    records_effects_df = pd.DataFrame.from_records(records_effects)\n",
    "    records_effects_df.drop_duplicates(subset=['Name', 'Model', 'Label', 'Feature'], inplace=True)\n",
    "\n",
    "    records_between_models_df.to_excel('tables/comparison_between_models.xlsx', index=False)\n",
    "\n",
    "    names = set(records_between_models_df['Name'].tolist())\n",
    "\n",
    "    records_effects_df.to_excel('tables/effects_between_models.xlsx', index=False)\n",
    "\n",
    "    if heatmap:\n",
    "        fig, ax = plt.subplots(2, len(names), figsize=(3*len(names), 6), squeeze=False)\n",
    "\n",
    "        for i, name in enumerate(names):\n",
    "\n",
    "            ax[0, i].set_title(name.capitalize())\n",
    "\n",
    "            sns.heatmap(records_between_models_df.query(f'Name == \"{name}\"').pivot(index='Model1', columns='Model2', values='Effect Spearman Correlation'),\n",
    "                annot=True, fmt='.2f', ax=ax[0, i],\n",
    "                cbar=(i == len(names) - 1),\n",
    "                # cbar_kws={'label': 'Spearman Correlation'},\n",
    "                vmin=-1, vmax=1)\n",
    "            \n",
    "            ax[0, i].set_xlabel('')\n",
    "            ax[0, i].set_ylabel('')\n",
    "\n",
    "\n",
    "            sns.heatmap(records_between_models_df.query(f'Name == \"{name}\"').pivot(index='Model1', columns='Model2', values='TV Distance'),\n",
    "                annot=True, fmt='.2f', ax=ax[1, i],\n",
    "                cbar=(i == len(names) - 1),\n",
    "                # cbar_kws={'label': 'TV Distance'},\n",
    "                vmin=0, vmax=1)\n",
    "            \n",
    "            ax[1, i].set_xlabel('')\n",
    "            ax[1, i].set_ylabel('')\n",
    "\n",
    "            sns.despine(ax=ax[0, i])\n",
    "            sns.despine(ax=ax[1, i])\n",
    "        \n",
    "        ax[0, 0].set_ylabel('Spearman Correlation')\n",
    "        ax[1, 0].set_ylabel('TV Distance')\n",
    "\n",
    "        for i in range(ax.shape[0]-1):\n",
    "            for j in range(ax.shape[1]):\n",
    "                ax[i, j].set_xticklabels([])\n",
    "\n",
    "        for j in range(1, ax.shape[1]):\n",
    "            for i in range(ax.shape[0]):\n",
    "                ax[i, j].set_yticklabels([])\n",
    "\n",
    "\n",
    "    else:\n",
    "\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 3), squeeze=False)\n",
    "\n",
    "        sns.barplot(x='Model2', y='Effect Spearman Correlation', hue='Name', data=records_between_models_df, ax=ax[0, 0], palette=palette)\n",
    "        ax[0, 0].set_xlabel('')\n",
    "        ax[0, 0].set_ylabel('Spearman Correlation')\n",
    "        # remove legend \n",
    "        ax[0, 0].legend_.remove()\n",
    "\n",
    "        sns.barplot(x='Model2', y='TV Distance', hue='Name', data=records_between_models_df, ax=ax[0, 1], palette=palette)\n",
    "        ax[0, 1].set_xlabel('')\n",
    "        ax[0, 1].set_ylabel('TV Distance')\n",
    "\n",
    "        # move legend to the right\n",
    "        ax[0, 1].legend_.remove()\n",
    "        ax[0, 1].legend(loc='upper left', bbox_to_anchor=(1, 1), title='Name')\n",
    "\n",
    "        sns.despine(ax=ax[0, 0])\n",
    "        sns.despine(ax=ax[0, 1])\n",
    "\n",
    "        ax[0, 1].set_ylim(0, 1)\n",
    "        ax[0, 0].set_ylim(-1, 1)\n",
    "\n",
    "    fig.suptitle(suptitle)\n",
    "    fig.supxlabel(supxlabel)\n",
    "    fig.supylabel(supylabel)\n",
    "\n",
    "    fig.subplots_adjust(top=0.85)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(outfile, dpi=300, bbox_inches='tight')\n",
    "\n",
    "def pretty_print_regression_table(filenames, outfile, full=False):\n",
    "\n",
    "    if isinstance(filenames, str):\n",
    "        filenames = [filenames]\n",
    "\n",
    "    regression_table_df = pd.concat([pd.read_excel(filename) for filename in filenames])\n",
    "\n",
    "    regression_table_df = regression_table_df.query('`Independent Variable` != \"[]\"')\n",
    "\n",
    "    table_rows_df = []\n",
    "    table_rows_ame_df = []\n",
    "\n",
    "    ego_row = True\n",
    "\n",
    "    temperature2idx = {}\n",
    "\n",
    "\n",
    "    for i, row in regression_table_df.iterrows():\n",
    "        temp = {}\n",
    "        temp_ame = {}\n",
    "        if row['Ego'] == -1:\n",
    "            ego_row = False\n",
    "        else:\n",
    "            temp['Ego'] = row['Ego']\n",
    "            temp_ame['Ego'] = row['Ego']\n",
    "        temp['Temperature'] = f\"{float(row['Temperature']):.1f}\"\n",
    "        temp_ame['Temperature'] = f\"{float(row['Temperature']):.1f}\"\n",
    "\n",
    "        if row['Temperature'] not in temperature2idx:\n",
    "            temperature2idx[row['Temperature']] = len(temperature2idx)\n",
    "\n",
    "        independent_variables = ast.literal_eval(row['Independent Variable'])\n",
    "\n",
    "        if (not full and len(independent_variables) ==  3) or full:\n",
    "\n",
    "            p_values = ast.literal_eval(row['P-values'])\n",
    "            coefficients = ast.literal_eval(row['Coefficients'])\n",
    "            standard_errors = ast.literal_eval(row['Standard Errors'])\n",
    "            ame = ast.literal_eval(row['AME']) if 'AME' in row else None\n",
    "            sdame = ast.literal_eval(row['SE AME']) if 'SE AME' in row else None\n",
    "\n",
    "            for j, feat_name in enumerate(independent_variables):\n",
    "                stars = '***' if float(p_values[j]) < 0.001 else '**' if float(p_values[j]) < 0.01 else '*' if float(p_values[j]) < 0.05 else ''\n",
    "                temp[f'{feat_name.replace(\"_\", \" \").capitalize()}'] = f\"{float(coefficients[j]):.2f}{stars} ({float(standard_errors[j]):.1g})\"\n",
    "                temp_ame[f'{feat_name.replace(\"_\", \" \").capitalize()}'] = f\"{float(ame[j]):.2f} ({float(sdame[j]):.1g})\"\n",
    "\n",
    "            temp['Log Likelihood'] = f\"{row['Log Likelihood']:,.2f}\"\n",
    "            temp['AIC'] = f'{2 * (len(independent_variables) + 1) - 2 * row[\"Log Likelihood\"]:,.2f}'\n",
    "\n",
    "            table_rows_df.append(temp)\n",
    "            table_rows_ame_df.append(temp_ame)\n",
    "\n",
    "    table_rows_df = pd.DataFrame.from_records(table_rows_df, columns=['Ego'] if ego_row else [] +  ['Temperature', 'Degree', 'Common attributes', 'Common neighbors', 'Log Likelihood', 'AIC'])\n",
    "    table_rows_ame_df = pd.DataFrame.from_records(table_rows_ame_df, columns=['Ego'] if ego_row else [] +  ['Temperature', 'Degree', 'Common attributes', 'Common neighbors'])\n",
    "    table_rows_df = table_rows_df.fillna(' ')\n",
    "    table_rows_ame_df = table_rows_ame_df.fillna(' ')\n",
    "\n",
    "    table_rows_df.to_latex(outfile, index=False, escape=True, column_format='lcccccc')\n",
    "\n",
    "    table_rows_ame_df.to_latex(outfile.replace('.tex', '_ame.tex'), index=False, escape=True, column_format='lcccccc')\n",
    "\n",
    "def modularity_change(filenames, subgraph=False):\n",
    "\n",
    "    for filename in filenames:\n",
    "\n",
    "        with open(filename) as f:\n",
    "            lines = f.read().splitlines()\n",
    "\n",
    "        for line in lines:\n",
    "            temp = json.loads(line)\n",
    "\n",
    "            G0 = nx.from_dict_of_dicts(temp['graphs'][0])\n",
    "            G1 = nx.from_dict_of_dicts(temp['graphs'][-1])\n",
    "\n",
    "            if subgraph:\n",
    "                H = nx.difference(G1, G0)\n",
    "                H.remove_nodes_from(list(nx.isolates(H)))                \n",
    "                G0 = nx.subgraph(G0, H.nodes())\n",
    "                G1 = nx.subgraph(G1, H.nodes())\n",
    "\n",
    "            modularities0 = []\n",
    "            modularities1 = []\n",
    "\n",
    "            for seed in range(10):\n",
    "                communities0 = nx.community.louvain_communities(G0, seed=seed)\n",
    "                modularities0.append(nx.community.modularity(G0, communities0))\n",
    "\n",
    "                communities1 = nx.community.louvain_communities(G1, seed=seed)\n",
    "                modularities1.append(nx.community.modularity(G1, communities1))\n",
    "\n",
    "\n",
    "            t, p = stats.ttest_ind(modularities0, modularities1, equal_var=False, alternative='less')\n",
    "\n",
    "            print(f'{temp[\"name\"]}, {temp[\"temperature\"]}, T-test: {t}, p-value: {p}')\n",
    "\n",
    "def lcc(G):\n",
    "    Gcc = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "    return G.subgraph(Gcc[0])\n",
    "\n",
    "def small_worldness(filenames, name, dataloader_fn, subgraph=False):\n",
    "\n",
    "    networks = dataloader_fn()\n",
    "\n",
    "    G_initial = networks[list(networks.keys())[0]]\n",
    "\n",
    "    # LCC subgraph\n",
    "    G_initial = lcc(G_initial)\n",
    "\n",
    "    average_shortest_path_length_initial = nx.average_shortest_path_length(G_initial)\n",
    "    clustering_coefficient_initial = nx.average_clustering(G_initial)\n",
    "\n",
    "    for filename in filenames:\n",
    "\n",
    "            with open(filename) as f:\n",
    "                lines = f.read().splitlines()\n",
    "\n",
    "            for line in lines:\n",
    "                temp = json.loads(line)\n",
    "\n",
    "                G0 = nx.from_dict_of_dicts(temp['graphs'][0])\n",
    "                G1 = nx.from_dict_of_dicts(temp['graphs'][-1])\n",
    "                G1 = lcc(G1)\n",
    "                \n",
    "                average_shortest_path_length = nx.average_shortest_path_length(G1)\n",
    "                clustering_coefficient = nx.average_clustering(G1)\n",
    "\n",
    "                average_shortest_path_length_initial_change = (average_shortest_path_length - average_shortest_path_length_initial) / average_shortest_path_length_initial * 100\n",
    "                clustering_coefficient_initial_change = (clustering_coefficient - clustering_coefficient_initial) / clustering_coefficient_initial * 100\n",
    "\n",
    "                print(f'{temp[\"name\"]}, {temp[\"temperature\"]}, Average Shortest Path Length Change: {average_shortest_path_length_initial_change}, Clustering Coefficient Change: {clustering_coefficient_initial_change}')\n",
    "\n",
    "\n",
    "def graph_statistics_change(filenames, outfile, subgraph=False):\n",
    "\n",
    "    \n",
    "    records = []\n",
    "\n",
    "    for filename in filenames:\n",
    "            \n",
    "            basename1 = filename.split('+')\n",
    "        \n",
    "            if len(basename1) == 3:\n",
    "                model = basename1[-2] + '+' + basename1[-1]\n",
    "            elif len(basename1) == 2:\n",
    "                model = basename1[-1]\n",
    "\n",
    "\n",
    "            # remove file extension from model\n",
    "            model = model.replace('.jsonl', '')\n",
    "\n",
    "            with open(filename) as f:\n",
    "                lines = f.read().splitlines()\n",
    "\n",
    "            for line in lines:\n",
    "                temp = json.loads(line)\n",
    "\n",
    "                name = temp['name']\n",
    "\n",
    "\n",
    "                G0 = nx.from_dict_of_dicts(temp['graphs'][0])\n",
    "                G1 = nx.from_dict_of_dicts(temp['graphs'][-1])\n",
    "\n",
    "                print(f'Analyzing {name} (n = {len(G0)}, m = {len(G0.edges())}) at temperature {temp[\"temperature\"]} using model {model}')\n",
    "\n",
    "                degrees_G0 = np.array([d for _, d in G0.degree()])\n",
    "                degrees_G1 = np.array([d for _, d in G1.degree()])\n",
    "\n",
    "                # 2-sample KS test\n",
    "                ks_statistic, p_value = stats.ks_2samp(degrees_G0, degrees_G1)\n",
    "\n",
    "                # get sizes of connected components\n",
    "                cc_sizes_G0 = [len(c) for c in nx.connected_components(G0)]\n",
    "                cc_sizes_G1 = [len(c) for c in nx.connected_components(G1)]\n",
    "\n",
    "                # 2-sample KS test for connected components sizes\n",
    "                ks_statistic_cc, p_value_cc = stats.ks_2samp(cc_sizes_G0, cc_sizes_G1)\n",
    "\n",
    "                # distribution of singular values\n",
    "                if len(G0) < 10000:\n",
    "                    svd_G0 = np.linalg.svd(nx.to_numpy_array(G0), compute_uv=False)\n",
    "                    svd_G1 = np.linalg.svd(nx.to_numpy_array(G1), compute_uv=False)\n",
    "                else:\n",
    "                    svd_G0, _ = scipy.sparse.linalg.eigsh(nx.to_scipy_sparse_array(G0), k=10)\n",
    "                    svd_G1, _ = scipy.sparse.linalg.eigsh(nx.to_scipy_sparse_array(G1), k=10)\n",
    "\n",
    "                ks_statistic_svd, p_value_svd = stats.ks_2samp(svd_G0, svd_G1)\n",
    "\n",
    "                # distributions of local clustering coefficients\n",
    "                clustering_coeffs_G0 = np.array(list(nx.clustering(G0).values()))\n",
    "                clustering_coeffs_G1 = np.array(list(nx.clustering(G1).values()))\n",
    "\n",
    "                ks_statistic_clustering, p_value_clustering = stats.ks_2samp(clustering_coeffs_G0, clustering_coeffs_G1)\n",
    "\n",
    "                m0 = G0.number_of_edges()\n",
    "                m1 = G1.number_of_edges()\n",
    "\n",
    "                number_of_new_edges_added = abs(m1 - m0) / m0 * 100 \n",
    "\n",
    "                print(f'Name: {temp[\"name\"]}, Temperature: {temp[\"temperature\"]}')\n",
    "               \n",
    "                records.append({\n",
    "                    'Name' : temp['name'],\n",
    "                    'Model' : rename_models.get(model, model),\n",
    "                    'Temp' : temp['temperature'],\n",
    "                    'Degree Distribution (KS)' : ks_statistic,\n",
    "                    'Degree Distribution (P-value)' : p_value,\n",
    "                    'Sizes of CCs (KS)' : ks_statistic_cc,\n",
    "                    'Sizes of CCs (P-value)' : p_value_cc,\n",
    "                    'Adjacency Spectrum (KS)' : ks_statistic_svd,\n",
    "                    'Adjacency Spectrum (P-value)' : p_value_svd,\n",
    "                    'Local Clustering Coefficient (KS)' : ks_statistic_clustering,\n",
    "                    'Local Clustering Coefficient (P-value)' : p_value_clustering,\n",
    "                    'Number of New Edges Added (%)' : number_of_new_edges_added,\n",
    "                })\n",
    "\n",
    "    records_df = pd.DataFrame.from_records(records)\n",
    "\n",
    "    with open(outfile, 'w') as f:\n",
    "        f.write(records_df.to_latex(index=False, escape=True, column_format='lccccccccccc', float_format='%.1g'))\n",
    "\n",
    "\n",
    "def measure_relative_increase(filenames):\n",
    "\n",
    "    for filename in filenames:\n",
    "\n",
    "        with open(filename) as f:\n",
    "            lines = f.read().splitlines()\n",
    "\n",
    "        data = []\n",
    "\n",
    "        for line in lines:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "        for d in data:\n",
    "            total, count = 0, 0        \n",
    "            for results in d[\"results\"]:\n",
    "                for result in results:\n",
    "                    count += int(result['dropped'])\n",
    "                    total += 1          \n",
    "\n",
    "            accuracy = count / total * 100\n",
    "            random_guess = 100 / d[\"num_choices\"]\n",
    "            relative_increase = (accuracy - random_guess) / random_guess * 100\n",
    "\n",
    "            print(f'{d[\"name\"]}, {d[\"temperature\"]}, {d[\"simulation\"]}, Relative Increase in Accuracy % = {relative_increase}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Formation Experiments\n",
    "\n",
    "### Results of Table 1\n",
    "\n",
    "#### Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run simulations \n",
    "models = ['gpt-3.5-turbo', 'meta/meta-llama-3-70b-instruct', 'gpt-4o-mini', 'claude-3-5-sonnet-20240620']\n",
    "\n",
    "datasets = ['Caltech36', 'Swarthmore42', 'UChicago30', 'andorra', 'mobiled']\n",
    "temperatures = [0.5]\n",
    "\n",
    "num_egonets = -1\n",
    "egonets_radius = -1\n",
    "sample_egonets = False\n",
    "\n",
    "for model in models:\n",
    "    print(model)\n",
    "    for name in datasets:\n",
    "        for i, temperature in enumerate(temperatures, 1):\n",
    "            outfile = f'outputs/combined_model_{name.lower()}_{i}+{model.replace(\"/\", \"-\")}.jsonl'\n",
    "            if name == 'andorra':\n",
    "                dataloader_fn = lambda: dataloader.load_andorra(input_dir='datasets/andorra')\n",
    "                run_network_formation_experiment(name=name, num_nodes_samples=1000, num_simulations=1, outfile=outfile, temperatures=[temperature], method='llm', num_choices=1, num_samples=5, model=model, dataloader_fn=dataloader_fn)\n",
    "            elif name == 'mobiled':\n",
    "                dataloader_fn = lambda: dataloader.load_mobiled(input_dir='datasets/mobiled')\n",
    "                run_network_formation_experiment(name=name, num_nodes_samples=1000, num_simulations=1, outfile=outfile, temperatures=[temperature], method='llm', num_choices=1, num_samples=5, model=model, dataloader_fn=dataloader_fn)    \n",
    "            else:\n",
    "                dataloader_fn = lambda: dataloader.load_facebook100(input_dir='datasets/facebook100', name=name, num_egonets=num_egonets, egonets_radius=egonets_radius, sample_egonets=sample_egonets)\n",
    "                run_network_formation_experiment(name=name, num_nodes_samples=2000, num_simulations=1, outfile=outfile, temperatures=[temperature], method='llm', num_choices=1, num_samples=15, model=model, dataloader_fn=dataloader_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Tables Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulations\n",
    "models = ['gpt-3.5-turbo', 'meta/meta-llama-3-70b-instruct', 'gpt-4o-mini', 'claude-3-5-sonnet-20240620']\n",
    "\n",
    "datasets = ['Caltech36', 'Swarthmore42', 'UChicago30', 'andorra', 'mobiled']\n",
    "temperatures = [0.5]\n",
    "\n",
    "num_egonets = -1\n",
    "egonets_radius = -1\n",
    "sample_egonets = False\n",
    "\n",
    "# Generate and format regression tables\n",
    "for model in models:\n",
    "    for name in datasets:\n",
    "        for i in range(1, 1 + len(temperatures)):\n",
    "            generate_regression_table(f'outputs/combined_model_{name.lower()}_{i}+{model.replace(\"/\", \"-\")}.jsonl', f'tables/combined_model_{name.lower()}_{i}+{model.replace(\"/\", \"-\")}.xlsx')\n",
    "\n",
    "# # Export regression tables to LaTeX\n",
    "for model in models:\n",
    "    for name in datasets:\n",
    "        for full in [True, False]:\n",
    "            pretty_print_regression_table([f'tables/combined_model_{name.lower()}_{i}+{model.replace(\"/\", \"-\")}.xlsx' for i in range(1, 1 + len(temperatures))], f'tables/combined_model_{name.lower()}{\"_full\" if full else \"\"}+{model.replace(\"/\", \"-\")}.tex', full=full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature Robustness Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulations \n",
    "models = ['gpt-4-1106-preview']\n",
    "\n",
    "datasets = ['Caltech36', 'Swarthmore42', 'UChicago30']\n",
    "temperatures = [0.5, 1.0, 1.5]\n",
    "\n",
    "num_egonets = -1\n",
    "egonets_radius = -1\n",
    "sample_egonets = False\n",
    "\n",
    "for model in models:\n",
    "    print(model)\n",
    "    for name in datasets:\n",
    "        for i, temperature in enumerate(temperatures, 1):\n",
    "            outfile = f'outputs/combined_model_{name.lower()}_{i}+{model.replace(\"/\", \"-\")}.jsonl'\n",
    "            if name == 'andorra':\n",
    "                dataloader_fn = lambda: dataloader.load_andorra(input_dir='datasets/andorra')\n",
    "                run_network_formation_experiment(name=name, num_nodes_samples=1000, num_simulations=1, outfile=outfile, temperatures=[temperature], method='llm', num_choices=1, num_samples=5, model=model, dataloader_fn=dataloader_fn)\n",
    "            elif name == 'mobiled':\n",
    "                dataloader_fn = lambda: dataloader.load_mobiled(input_dir='datasets/mobiled')\n",
    "                run_network_formation_experiment(name=name, num_nodes_samples=1000, num_simulations=1, outfile=outfile, temperatures=[temperature], method='llm', num_choices=1, num_samples=5, model=model, dataloader_fn=dataloader_fn)    \n",
    "            else:\n",
    "                dataloader_fn = lambda: dataloader.load_facebook100(input_dir='datasets/facebook100', name=name, num_egonets=num_egonets, egonets_radius=egonets_radius, sample_egonets=sample_egonets)\n",
    "                run_network_formation_experiment(name=name, num_nodes_samples=2000, num_simulations=1, outfile=outfile, temperatures=[temperature], method='llm', num_choices=1, num_samples=15, model=model, dataloader_fn=dataloader_fn)\n",
    "\n",
    "\n",
    "\n",
    "temperatures = [0.5, 1.0, 1.5]\n",
    "for model in ['gpt-4-1106-preview']:\n",
    "    for name in ['Caltech36', 'Swarthmore42', 'UChicago30']:\n",
    "        for i in range(1, 1 + len(temperatures)):\n",
    "            dataloader_fn = lambda: dataloader.load_facebook100(input_dir='datasets/facebook100', name=name, num_egonets=num_egonets, egonets_radius=egonets_radius, sample_egonets=sample_egonets)\n",
    "            graph_statistics_change([f'outputs/combined_model_{name.lower()}_{i}+{model.replace(\"/\", \"-\")}.jsonl'], name, dataloader_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Context Window Experiments\n",
    "\n",
    "### Experiments with large context model `gpt-4.1-mini`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large context windows\n",
    "num_egonets = -1\n",
    "egonets_radius = -1\n",
    "sample_egonets = False\n",
    "\n",
    "table_outfiles_context_window = []\n",
    "\n",
    "models = ['gpt-4.1-mini']\n",
    "datasets = ['Caltech36', 'Swarthmore42', 'UChicago30', 'andorra', 'mobiled']\n",
    "temperatures = [0.5]\n",
    "\n",
    "num_samples = [50, 100]\n",
    "\n",
    "for model in models:\n",
    "    for num_sample in num_samples:\n",
    "        for name in datasets:\n",
    "            print('Model:', model, 'Num Samples:', num_sample, 'Dataset: ', name)\n",
    "            for i, temperature in enumerate(temperatures, 1):\n",
    "                outfile = f'outputs/combined_model_{name.lower()}_{i}_+{model.replace(\"/\", \"-\")}+{num_sample}.jsonl'\n",
    "                if name == 'andorra':\n",
    "                    dataloader_fn = lambda: dataloader.load_andorra(input_dir='datasets/andorra')\n",
    "                    run_network_formation_experiment(name=name, num_nodes_samples=1000, num_simulations=1, outfile=outfile, temperatures=[temperature], method='llm', num_choices=1, num_samples=num_sample, model=model, dataloader_fn=dataloader_fn)\n",
    "                elif name == 'mobiled':\n",
    "                    dataloader_fn = lambda: dataloader.load_mobiled(input_dir='datasets/mobiled')\n",
    "                    run_network_formation_experiment(name=name, num_nodes_samples=1000, num_simulations=1, outfile=outfile, temperatures=[temperature], method='llm', num_choices=1, num_samples=num_sample, model=model, dataloader_fn=dataloader_fn)\n",
    "                else:\n",
    "                    dataloader_fn = lambda: dataloader.load_facebook100(input_dir='datasets/facebook100', name=name, num_egonets=num_egonets, egonets_radius=egonets_radius, sample_egonets=sample_egonets)\n",
    "                    run_network_formation_experiment(name=name, num_nodes_samples=1000, num_simulations=1, outfile=outfile, temperatures=[temperature], method='llm', num_choices=1, num_samples=num_sample, model=model, dataloader_fn=dataloader_fn)\n",
    "\n",
    "                # table_outfiles_context_window.append(outfile)\n",
    "\n",
    "\n",
    "                generate_regression_table(outfile, f'tables/combined_model_{name.lower()}_{i}+{model.replace(\"/\", \"-\")}+{num_sample}.xlsx')\n",
    "\n",
    "# # Export regression tables to LaTeX\n",
    "for model in models:\n",
    "    for num_sample in num_samples:\n",
    "        for name in datasets:\n",
    "            for full in [True, False]:\n",
    "                pretty_print_regression_table([f'tables/combined_model_{name.lower()}_{i}+{model.replace(\"/\", \"-\")}+{num_sample}.xlsx' for i in range(1, 1 + len(temperatures))], f'tables/combined_model_{name.lower()}{\"_full\" if full else \"\"}+{model.replace(\"/\", \"-\")}+{num_sample}.tex', full=full)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate Selection Strategies Experiments\n",
    "\n",
    "### Simulations for the Recommendation System Candidate Selection Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run network formation experiments with selection strategies\n",
    "table_outfiles_selection_strategy_link_prediction = []\n",
    "table_outfiles_selection_strategy_uniform = []\n",
    "\n",
    "models = ['gpt-4o-mini', 'gpt-3.5-turbo', 'meta/meta-llama-3-70b-instruct', 'claude-3-5-sonnet-20240620']\n",
    "datasets = ['Caltech36', 'Swarthmore42', 'UChicago30', 'andorra', 'mobiled']\n",
    "\n",
    "temperatures = [0.5]\n",
    "\n",
    "sampling_strategies = ['link_prediction']\n",
    "\n",
    "num_egonets = -1\n",
    "egonets_radius = -1\n",
    "sample_egonets = False\n",
    "\n",
    "for model in models:\n",
    "    for sampling_strategy in sampling_strategies:\n",
    "        for name in datasets:\n",
    "            print('Model:', model, 'Sampling Strategy:', sampling_strategy, 'Dataset: ', name)\n",
    "            for i, temperature in enumerate(temperatures, 1):\n",
    "                outfile = f'outputs/combined_model_{name.lower()}_{i}_+{model.replace(\"/\", \"-\")}+{sampling_strategy}.jsonl'\n",
    "                outfile_uniform = f'outputs/combined_model_{name.lower()}_{i}+{model.replace(\"/\", \"-\")}.jsonl'\n",
    "\n",
    "                if name == 'andorra':\n",
    "                    dataloader_fn = lambda: dataloader.load_andorra(input_dir='datasets/andorra')\n",
    "                    run_network_formation_experiment(name=name, num_nodes_samples=1000, num_simulations=1, outfile=outfile, temperatures=[temperature], method='llm', num_choices=1, num_samples=5, model=model, dataloader_fn=dataloader_fn, sampling_strategy=sampling_strategy)\n",
    "                elif name == 'mobiled':\n",
    "                    dataloader_fn = lambda: dataloader.load_mobiled(input_dir='datasets/mobiled')\n",
    "                    run_network_formation_experiment(name=name, num_nodes_samples=1000, num_simulations=1, outfile=outfile, temperatures=[temperature], method='llm', num_choices=1, num_samples=5, model=model, dataloader_fn=dataloader_fn, sampling_strategy=sampling_strategy)\n",
    "                else:\n",
    "                    dataloader_fn = lambda: dataloader.load_facebook100(input_dir='datasets/facebook100', name=name, num_egonets=num_egonets, egonets_radius=egonets_radius, sample_egonets=sample_egonets)\n",
    "                    run_network_formation_experiment(name=name, num_nodes_samples=1000, num_simulations=1, outfile=outfile, temperatures=[temperature], method='llm', num_choices=1, num_samples=15, model=model, dataloader_fn=dataloader_fn, sampling_strategy=sampling_strategy)\n",
    "\n",
    "                table_outfiles_selection_strategy_link_prediction.append(outfile)\n",
    "                table_outfiles_selection_strategy_uniform.append(outfile_uniform)\n",
    "                generate_regression_table(outfile, f'tables/combined_model_{name.lower()}_{i}+{model.replace(\"/\", \"-\")}+{sampling_strategy}.xlsx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change of Graph Statistics for Different Sampling Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph statistics change\n",
    "print('Recommendation system')\n",
    "graph_statistics_change(table_outfiles_selection_strategy_link_prediction, 'tables/graph_statistics_change_selection_strategy_link_prediction.tex', subgraph=False)\n",
    "print('Link Prediction')\n",
    "graph_statistics_change(table_outfiles_selection_strategy_uniform, 'tables/graph_statistics_change_selection_strategy_uniform.tex', subgraph=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression table for the Recommendation System Candidate Selection Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LaTeX tables for selection strategies\n",
    "table_outfiles_selection_strategy_link_prediction = []\n",
    "table_outfiles_selection_strategy_uniform = []\n",
    "\n",
    "models = ['gpt-4o-mini', 'gpt-3.5-turbo', 'meta/meta-llama-3-70b-instruct', 'claude-3-5-sonnet-20240620']\n",
    "datasets = ['Caltech36', 'Swarthmore42', 'UChicago30', 'andorra', 'mobiled']\n",
    "\n",
    "temperatures = [0.5]\n",
    "\n",
    "sampling_strategies = ['link_prediction']\n",
    "\n",
    "num_egonets = -1\n",
    "egonets_radius = -1\n",
    "sample_egonets = False\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    for sampling_strategy in sampling_strategies:\n",
    "        for name in datasets:\n",
    "            for full in [True, False]:\n",
    "                pretty_print_regression_table([f'tables/combined_model_{name.lower()}_{i}+{model.replace(\"/\", \"-\")}+{sampling_strategy}.xlsx' for i in range(1, 1 + len(temperatures))], f'tables/combined_model_{name.lower()}{\"_full\" if full else \"\"}+{model.replace(\"/\", \"-\")}+{sampling_strategy}.tex', full=full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Alignment Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation System vs Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model alignment plots\n",
    "models = ['gpt-4o-mini', 'gpt-3.5-turbo', 'meta/meta-llama-3-70b-instruct', 'claude-3-5-sonnet-20240620']\n",
    "datasets = ['Caltech36', 'Swarthmore42', 'UChicago30', 'andorra', 'mobiled']\n",
    "\n",
    "temperatures = [0.5]\n",
    "\n",
    "sampling_strategies = ['link_prediction']\n",
    "\n",
    "num_egonets = -1\n",
    "egonets_radius = -1\n",
    "sample_egonets = False\n",
    "\n",
    "table_outfiles1 = []\n",
    "table_outfiles2 = []\n",
    "\n",
    "for sampling_strategy in sampling_strategies:\n",
    "    for model1 in models:\n",
    "        for model2 in models:\n",
    "            for name in datasets:\n",
    "                for i, temperature in enumerate(temperatures, 1):\n",
    "                    outfile1 = f'outputs/combined_model_{name.lower()}_{i}_+{model1.replace(\"/\", \"-\")}+{sampling_strategy}.jsonl'\n",
    "                    outfile2 = f'outputs/combined_model_{name.lower()}_{i}_+{model2.replace(\"/\", \"-\")}+{sampling_strategy}.jsonl'\n",
    "\n",
    "                    table_outfiles1.append(outfile1)\n",
    "                    table_outfiles2.append(outfile2)\n",
    "    \n",
    "\n",
    "compare_models(table_outfiles1, table_outfiles2, bias=True, log_transform=True, exclude_log=[], heatmap=True, supxlabel='Recommendation System', supylabel='Recommendation System', outfile='figures/comparison_between_models_recsys.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['gpt-3.5-turbo', 'meta/meta-llama-3-70b-instruct', 'gpt-4o-mini', 'claude-3-5-sonnet-20240620']\n",
    "\n",
    "datasets = ['Caltech36', 'Swarthmore42', 'UChicago30', 'andorra', 'mobiled']\n",
    "temperatures = [0.5]\n",
    "\n",
    "num_egonets = -1\n",
    "egonets_radius = -1\n",
    "sample_egonets = False\n",
    "\n",
    "table_outfiles1 = []\n",
    "table_outfiles2 = []\n",
    "\n",
    "for model1 in models:\n",
    "    for model2 in models:\n",
    "        for name in datasets:\n",
    "            for i, temperature in enumerate(temperatures, 1):\n",
    "                outfile1 = f'outputs/combined_model_{name.lower()}_{i}+{model1.replace(\"/\", \"-\")}.jsonl'\n",
    "                outfile2 = f'outputs/combined_model_{name.lower()}_{i}+{model2.replace(\"/\", \"-\")}.jsonl'\n",
    "\n",
    "                table_outfiles1.append(outfile1)\n",
    "                \n",
    "                table_outfiles2.append(outfile2)\n",
    "\n",
    "\n",
    "compare_models(table_outfiles1, table_outfiles2, bias=True, log_transform=True, exclude_log=[], heatmap=True, supxlabel='Uniform', supylabel='Uniform', outfile='figures/comparison_between_models_uniform.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['gpt-3.5-turbo', 'meta/meta-llama-3-70b-instruct', 'gpt-4o-mini', 'claude-3-5-sonnet-20240620']\n",
    "\n",
    "datasets = ['Caltech36', 'Swarthmore42', 'UChicago30', 'andorra', 'mobiled']\n",
    "temperatures = [0.5]\n",
    "\n",
    "num_egonets = -1\n",
    "egonets_radius = -1\n",
    "sample_egonets = False\n",
    "sampling_strategies = ['link_prediction']\n",
    "\n",
    "table_outfiles1 = []\n",
    "table_outfiles2 = []\n",
    "\n",
    "for sampling_strategy in sampling_strategies:\n",
    "    for model1 in models:\n",
    "        for model2 in models:\n",
    "            for name in datasets:\n",
    "                for i, temperature in enumerate(temperatures, 1):\n",
    "                    outfile1 = f'outputs/combined_model_{name.lower()}_{i}+{model1.replace(\"/\", \"-\")}.jsonl'\n",
    "                    outfile2 = f'outputs/combined_model_{name.lower()}_{i}_+{model2.replace(\"/\", \"-\")}+{sampling_strategy}.jsonl'\n",
    "\n",
    "                    table_outfiles1.append(outfile1)\n",
    "                    table_outfiles2.append(outfile2)\n",
    "\n",
    "\n",
    "compare_models(table_outfiles1, table_outfiles2, bias=True, log_transform=True, exclude_log=[], heatmap=True, supxlabel='Uniform Sampling', supylabel='Recommendation System', outfile='figures/comparison_between_models_uniform_recsys.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
