{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import scipy.stats as stats\n",
    "\n",
    "import seaborn as sns\n",
    "import json\n",
    "import openai\n",
    "import random\n",
    "import os\n",
    "import collections\n",
    "import itertools\n",
    "import copy\n",
    "import dataloader\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "import netgraph\n",
    "import scipy\n",
    "import ast\n",
    "from utils import get_response, summarize_reasons\n",
    "\n",
    "MEDIUM_SIZE = 20\n",
    "SMALL_SIZE = 0.85 * MEDIUM_SIZE\n",
    "BIGGER_SIZE = 1.5 * MEDIUM_SIZE\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "def network_growth(G0, temperature, num_choices=1, method='llm', num_samples=-1, num_nodes_samples=-1, model='gpt-4-1106-preview'):\n",
    "    # Set seed\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)    \n",
    "\n",
    "    # Copy the ground truth graph\n",
    "    G = G0.copy()\n",
    "   \n",
    "    Gs = [G.copy()]\n",
    "\n",
    "    profiles = nx.get_node_attributes(G, 'features')\n",
    "\n",
    "    # Edges to drop\n",
    "    dropped_edges = []\n",
    "\n",
    "    if num_nodes_samples > 0:\n",
    "        nodes = random.sample(G.nodes(), min(len(G), num_nodes_samples))\n",
    "    else:\n",
    "        nodes = G.nodes()\n",
    "\n",
    "    # Drop one neighbor for each node\n",
    "    for v in nodes:\n",
    "        dropped_v_edges = []\n",
    "        for _ in range(num_choices):\n",
    "            if len(list(G.neighbors(v))) > 0:\n",
    "                \n",
    "                while True:\n",
    "                    u = random.choice(list(G.neighbors(v)))\n",
    "                    if (v, u) not in dropped_edges:\n",
    "                        dropped_v_edges.append((v, u))\n",
    "                        G.remove_edge(v, u)\n",
    "                        break\n",
    "\n",
    "        dropped_edges.append(dropped_v_edges)\n",
    "\n",
    "    Gs = [G.copy()]\n",
    "    results = []\n",
    "    candidates = []\n",
    "\n",
    "\n",
    "    for i, t in enumerate(nodes):\n",
    "\n",
    "        if method == 'llm':\n",
    "            result, candidate = select_neighbor(G, t, profiles, temperature, num_choices=len(dropped_edges[i]), dropped_nodes=[u for (_, u) in dropped_edges[i]], num_samples=num_samples, model=model)\n",
    "\n",
    "            if result:\n",
    "                for r in result:\n",
    "                    v = r['name']\n",
    "                    r['edge'] = (t, v)\n",
    "                    G.add_edge(t, v, similarity=r['similarity'])\n",
    "                results.append(result)\n",
    "\n",
    "            candidates.append(candidate)\n",
    "        if method == 'ground_truth':\n",
    "            if num_samples > num_choices:\n",
    "                choice_set = random.sample([v for v in G.nodes() if v != t], num_samples - num_choices)\n",
    "            else:\n",
    "                choice_set = [v for v in G.nodes() if v != t]\n",
    "            \n",
    "            new_nodes = [e[1] for e in dropped_edges[i]]\n",
    "\n",
    "            choice_set = choice_set + new_nodes\n",
    "\n",
    "\n",
    "            result = []\n",
    "\n",
    "            for v in new_nodes:\n",
    "\n",
    "                profiles[t]['neighbors'] = list(G.neighbors(t))\n",
    "                profiles[v]['neighbors'] = list(G.neighbors(v))\n",
    "                profiles[t]['degree'] = len(profiles[t]['neighbors'])\n",
    "                profiles[v]['degree'] = len(profiles[v]['neighbors'])\n",
    "\n",
    "                similarity = measure_similarity(profiles[t], profiles[v])\n",
    "                G.add_edge(t, v, similarity=similarity, weight=similarity['common_attributes'])\n",
    "            \n",
    "                result.append({'name' : v, 'similarity' : similarity, 'reason' : method, 'dropped' : True})\n",
    "\n",
    "            candidate = []\n",
    "\n",
    "            for v in choice_set:\n",
    "                profiles[t]['neighbors'] = list(G.neighbors(t))\n",
    "                profiles[v]['neighbors'] = list(G.neighbors(v))\n",
    "                profiles[t]['degree'] = len(profiles[t]['neighbors'])\n",
    "                profiles[v]['degree'] = len(profiles[v]['neighbors'])\n",
    "\n",
    "                similarity = measure_similarity(profiles[t], profiles[v])\n",
    "                candidate.append({'name' : v, 'similarity' : similarity, 'reason' : method})\n",
    "\n",
    "            candidates.append(candidate)\n",
    "            results.append(result)\n",
    "\n",
    "            print(f'Node: {t}, Links: {result}, Candidates: {candidate}')\n",
    "\n",
    "        Gs.append(G.copy())\n",
    "\n",
    "    return Gs, results, candidates\n",
    "\n",
    "def fit_dcm(results):\n",
    "\n",
    "    similarities = [r['similarity'] for result in results for r in result]\n",
    "    similarities_df = pd.DataFrame.from_records(similarities)\n",
    "    similarities_df = sm.add_constant(similarities_df)\n",
    "\n",
    "    outcomes = np.array([r['edge'][1] for result in results for r in result])\n",
    "\n",
    "    print(similarities_df)\n",
    "\n",
    "    mnl_model = sm.MNLogit(outcomes, similarities_df)\n",
    "    mnl_results = mnl_model.fit()\n",
    "\n",
    "    print(mnl_results.summary())\n",
    "\n",
    "    return mnl_results\n",
    "\n",
    "def measure_similarity(profile1, profile2):\n",
    "\n",
    "    similarity = {\n",
    "        'common_attributes' : 0,\n",
    "        'common_neighbors' : len(set(profile1['neighbors']) & set(profile2['neighbors'])),\n",
    "        'degree' : profile2['degree'],\n",
    "    }\n",
    "\n",
    "    for k in profile1.keys():\n",
    "        if k != 'name' and k != 'neighbors' and k in profile2.keys():\n",
    "            if isinstance(profile1[k], list):\n",
    "                similarity['common_attributes'] += len(set(profile1[k]) & set(profile2[k]))\n",
    "            elif profile1[k] == profile2[k]:\n",
    "                similarity['common_attributes'] += 1\n",
    "        \n",
    "    return similarity\n",
    "\n",
    "\n",
    "def select_neighbor(G, t, profiles, temperature, num_choices=1, num_samples=-1, dropped_nodes=[], model='gpt-4-1106-preview'):\n",
    "\n",
    "    if num_samples > 0:\n",
    "        choice_set = random.sample([v for v in G.nodes() if v != t and v not in G.neighbors(t)], max(0, num_samples - len(dropped_nodes))) + dropped_nodes\n",
    "    else:\n",
    "        choice_set = [v for v in G.nodes() if v != t and v not in G.neighbors(t)]\n",
    "\n",
    "    candidate_profiles = []\n",
    "\n",
    "    for v in choice_set + [t]:\n",
    "        profiles[v]['neighbors'] = list(G.neighbors(v))\n",
    "        profiles[v]['degree'] = len(profiles[v]['neighbors']) \n",
    "        profiles[v]['name'] = v                 \n",
    "        candidate_profiles.append(profiles[v])\n",
    "\n",
    "    random.shuffle(candidate_profiles)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    # Task\n",
    "    Your task is to select a set of people to be friends with.\n",
    "\n",
    "    # Profile\n",
    "    Your profile is given below after chevrons:\n",
    "    <PROFILE>\n",
    "    {json.dumps(profiles[t])}\n",
    "    </PROFILE>\n",
    "\n",
    "    # Candidate Profiles\n",
    "    The cadidate profiles to be friends with are given below after chevrons:\n",
    "\n",
    "    <PROFILES>\n",
    "    {json.dumps(candidate_profiles)}\n",
    "    </PROFILES>\n",
    "\n",
    "    # Output\n",
    "    The output should be given a list of JSON objects with the following structure\n",
    "\n",
    "    [\n",
    "        {{\n",
    "            \"name\" : name of the person you selected,\n",
    "            \"reason\" : reason for selecting the person\n",
    "        }}, ...\n",
    "    ]\n",
    "\n",
    "    # Notes\n",
    "    * The output must be a list of JSON objects ranked in the order of preference.\n",
    "    * You can make at most {num_choices} selection{'s' if num_choices > 1 else ''}.\n",
    "    * Your output must be contained within the json markdown cue.\n",
    "    \n",
    "    ```json\n",
    "    \"\"\"   \n",
    "    \n",
    "    for _ in range(10):\n",
    "        try:\n",
    "            ans = get_response(prompt, temperature=temperature, model=model)\n",
    "            try:\n",
    "                results = json.loads(ans.split('```')[0])\n",
    "            except:\n",
    "                results = json.loads(ans.split('```json')[1].split('```')[0])\n",
    "\n",
    "            filtered_results = []\n",
    "            for result in results:\n",
    "                v = result['name']\n",
    "                if v in G.nodes():\n",
    "                    result['similarity'] = measure_similarity(profiles[t], profiles[v])\n",
    "                    filtered_results.append(result)\n",
    "\n",
    "                    result['dropped'] = v in dropped_nodes\n",
    "\n",
    "            print(f'Node: {t}, Links: {filtered_results}')\n",
    "\n",
    "            candidates = []\n",
    "\n",
    "            for candidate_profile in candidate_profiles:\n",
    "                similarity = measure_similarity(profiles[t], candidate_profile)\n",
    "                candidates.append({'name' : candidate_profile['name'], 'similarity' : similarity})\n",
    "\n",
    "            return filtered_results, candidates\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    return [], []\n",
    "\n",
    "\n",
    "def run_network_formation_experiment(name, num_simulations, outfile, temperatures, method, num_choices, num_samples, num_nodes_samples, model, dataloader_fn):\n",
    "    networks = dataloader_fn()\n",
    "    \n",
    "    saved_scenarios = set()\n",
    "\n",
    "    if os.path.exists(outfile):\n",
    "        with open(outfile) as f:\n",
    "            lines = f.read().splitlines()\n",
    "\n",
    "            for line in lines:\n",
    "                scenario = json.loads(line)\n",
    "                saved_scenarios.add((scenario['name'], scenario['ego'], scenario['simulation'], scenario['temperature'], scenario['num_samples'], scenario['num_choices']))\n",
    "\n",
    "\n",
    "    f = open(outfile, 'a+')\n",
    "\n",
    "\n",
    "    for ego, G0 in networks.items():\n",
    "        for i in range(num_simulations):\n",
    "            for temperature in temperatures:\n",
    "                if (name, ego, i, temperature, num_samples, num_choices) in saved_scenarios:\n",
    "                    print(f'Skipping simulation for name={name}, ego={ego}, i={i}, temperature={temperature}, num_choices={num_choices}, num_samples={num_samples}, method={method}')\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f'Running simulation for name={name}, ego={ego}, i={i}, temperature={temperature}, num_choices={num_choices}, num_samples={num_samples}, method={method}')\n",
    "\n",
    "                    Gs, results, candidates = network_growth(G0, temperature=temperature, method=method, num_choices=num_choices, num_samples=num_samples, num_nodes_samples=num_nodes_samples, model=model)\n",
    "\n",
    "                    temp = {\n",
    "                        'name' : name,\n",
    "                        'ego' : ego,\n",
    "                        'temperature' : temperature,\n",
    "                        'simulation' : i,\n",
    "                        'num_choices' : num_choices,\n",
    "                        'num_samples' : num_samples,\n",
    "                        'graphs' : [nx.to_dict_of_dicts(G) for G in [Gs[0], Gs[-1]]],\n",
    "                        'results' : results,\n",
    "                        'candidates' : candidates,\n",
    "                        'model' : model\n",
    "                    }    \n",
    "\n",
    "                    f.write(json.dumps(temp) + '\\n')            \n",
    "\n",
    "                if method != 'llm':\n",
    "                    break\n",
    "\n",
    "    f.close()\n",
    "\n",
    "def draw_graph(G, ax, communities=None, palette=None):\n",
    "\n",
    "    pos = nx.spring_layout(G)\n",
    "\n",
    "    netgraph.Graph(G, node_layout=pos, node_color='#d35400', node_size=2.5, edge_color='#34495e', edge_width=1, ax=ax)\n",
    "\n",
    "    ax.set_axis_off()\n",
    "\n",
    "\n",
    "def generate_regression_table(filename, outfile, bias=True, log_transform=True, exclude_log=[]):\n",
    "\n",
    "    palette = ['#d35400', '#34495e', '#2980b9', '#e67e22', '#f1c40f', '#7f8c8d', '#27ae60', '#16a085', '#bdc3c7', '#1abc9c', '#2ecc71', '#3498db', '#9b59b6', '#8e44ad', '#ecf0f1']\n",
    "\n",
    "    with open(filename) as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for line in lines:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "    feature_names = ['degree', 'common_attributes', 'common_neighbors']\n",
    "\n",
    "    regression_table_df = []\n",
    "    \n",
    "    names = set([d['name'] for d in data])\n",
    "\n",
    "\n",
    "    for d in data:\n",
    "        # Gs = []\n",
    "        # for graph in d['graphs']:\n",
    "        #     G = nx.from_dict_of_dicts(graph)\n",
    "        #     Gs.append(G)\n",
    "\n",
    "        log_likelihoods = {}\n",
    "\n",
    "        for num_features in range(len(feature_names) + 1):\n",
    "            for feature_combination in itertools.combinations(feature_names, num_features):\n",
    "                feature_combination = list(feature_combination)\n",
    "                theta, relative_probabilities, log_Z_mean, log_likelihood, standard_errors, choices, choice_sets = fit_discrete_choice_model(d['results'], d['candidates'], feature_names=feature_combination, bias=bias, log_transform=log_transform, exclude_log=exclude_log)\n",
    "                \n",
    "                print(feature_combination, theta, standard_errors)\n",
    "\n",
    "                temp = {\n",
    "                    'Name' : d[\"name\"],\n",
    "                    'Ego' : d[\"ego\"],\n",
    "                    'Temperature' : d[\"temperature\"],\n",
    "                    'Simulation' : d[\"simulation\"],\n",
    "                    'Number of Choices' : d[\"num_choices\"],\n",
    "                    'Number of Samples' : d[\"num_samples\"],\n",
    "                    'Independent Variable' : feature_combination,\n",
    "                    'Coefficients' : theta[:-1].tolist(),\n",
    "                    'Standard Errors' : standard_errors[:-1].tolist(),\n",
    "                    'Log Likelihood' : log_likelihood,\n",
    "                }\n",
    "\n",
    "                log_likelihoods[tuple(sorted(feature_combination))] = log_likelihood\n",
    "                p_values = np.array([1 - stats.chi2.cdf(2 * (log_likelihood - log_likelihoods[tuple(sorted(feature_combination[:i] + feature_combination[i + 1:]))]), 1) for i in range(len(feature_combination))])\n",
    "\n",
    "                temp['P-values'] = p_values.tolist()\n",
    "\n",
    "                regression_table_df.append(temp)\n",
    "             \n",
    "                if len(feature_combination) == 3:\n",
    "                    plot_choices(d['results'], d['candidates'], feature_names=feature_combination)\n",
    "\n",
    "    regression_table_df = pd.DataFrame.from_records(regression_table_df)\n",
    "\n",
    "    regression_table_df.to_excel(outfile)\n",
    "\n",
    "\n",
    "def pretty_print_regression_table(filenames, outfile, full=False):\n",
    "\n",
    "    if isinstance(filenames, str):\n",
    "        filenames = [filenames]\n",
    "\n",
    "    regression_table_df = pd.concat([pd.read_excel(filename) for filename in filenames])\n",
    "\n",
    "    regression_table_df = regression_table_df.query('`Independent Variable` != \"[]\"')\n",
    "\n",
    "    table_rows_df = []\n",
    "\n",
    "    ego_row = True\n",
    "\n",
    "    temperature2idx = {}\n",
    "\n",
    "\n",
    "    for i, row in regression_table_df.iterrows():\n",
    "        temp = {}\n",
    "        if row['Ego'] == -1:\n",
    "            ego_row = False\n",
    "        else:\n",
    "            temp['Ego'] = row['Ego']\n",
    "        temp['Temperature'] = f\"{float(row['Temperature']):.1f}\"\n",
    "\n",
    "        if row['Temperature'] not in temperature2idx:\n",
    "            temperature2idx[row['Temperature']] = len(temperature2idx)\n",
    "\n",
    "        independent_variables = ast.literal_eval(row['Independent Variable'])\n",
    "\n",
    "        if (not full and len(independent_variables) ==  3) or full:\n",
    "\n",
    "            p_values = ast.literal_eval(row['P-values'])\n",
    "            coefficients = ast.literal_eval(row['Coefficients'])\n",
    "            standard_errors = ast.literal_eval(row['Standard Errors'])\n",
    "\n",
    "            for j, feat_name in enumerate(independent_variables):\n",
    "                stars = '***' if float(p_values[j]) < 0.001 else '**' if float(p_values[j]) < 0.01 else '*' if float(p_values[j]) < 0.05 else ''\n",
    "                temp[f'{feat_name.replace(\"_\", \" \").capitalize()}'] = f\"{float(coefficients[j]):.2f}{stars} ({float(standard_errors[j]):.1g})\"\n",
    "\n",
    "            temp['Log Likelihood'] = f\"{row['Log Likelihood']:,.2f}\"\n",
    "            temp['AIC'] = f'{2 * (len(independent_variables) + 1) - 2 * row[\"Log Likelihood\"]:,.2f}'\n",
    "\n",
    "            table_rows_df.append(temp)\n",
    "\n",
    "\n",
    "    table_rows_df = pd.DataFrame.from_records(table_rows_df, columns=['Ego'] if ego_row else [] +  ['Temperature', 'Degree', 'Common attributes', 'Common neighbors', 'Log Likelihood', 'AIC'])\n",
    "    table_rows_df = table_rows_df.fillna(' ')\n",
    "\n",
    "    table_rows_df.to_latex(outfile, index=False, escape=True, column_format='lcccccc')\n",
    "\n",
    "def plot_coefficients(filenames, outfile):\n",
    "\n",
    "    palette = ['#d35400', '#34495e', '#2980b9', '#e67e22', '#f1c40f', '#7f8c8d', '#27ae60', '#16a085', '#bdc3c7', '#1abc9c', '#2ecc71', '#3498db', '#9b59b6', '#8e44ad', '#ecf0f1']\n",
    "\n",
    "    if isinstance(filenames, str):\n",
    "        filenames = [filenames]\n",
    "\n",
    "    regression_table_df = pd.concat([pd.read_excel(filename) for filename in filenames])\n",
    "\n",
    "    regression_table_df = regression_table_df.query('`Independent Variable` != \"[]\"')\n",
    "\n",
    "    table_rows_df = []\n",
    "\n",
    "    ego_row = True\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    temperature2idx = {}\n",
    "\n",
    "    name2idx = {}\n",
    "\n",
    "    variables2latex = {\n",
    "        'degree' : '$\\\\hat \\\\theta_{PA}$',\n",
    "        'common_attributes' : '$\\\\hat \\\\theta_{H}$',\n",
    "        'common_neighbors' : '$\\\\hat \\\\theta_{TC}$',\n",
    "    }\n",
    "\n",
    "    for i, row in regression_table_df.iterrows():\n",
    "        if row['Name'] not in name2idx:\n",
    "            name2idx[row['Name']] = len(name2idx)\n",
    "\n",
    "    for i, row in regression_table_df.iterrows():\n",
    "        temp = {}\n",
    "        if row['Ego'] == -1:\n",
    "            ego_row = False\n",
    "        else:\n",
    "            temp['Ego'] = row['Ego']\n",
    "        temp['Temperature'] = row['Temperature']\n",
    "\n",
    "        if row['Temperature'] not in temperature2idx:\n",
    "            temperature2idx[row['Temperature']] = len(temperature2idx)\n",
    "\n",
    "        independent_variables = ast.literal_eval(row['Independent Variable'])\n",
    "\n",
    "        if len(independent_variables) ==  3:\n",
    "\n",
    "            p_values = ast.literal_eval(row['P-values'])\n",
    "            coefficients = ast.literal_eval(row['Coefficients'])\n",
    "            standard_errors = ast.literal_eval(row['Standard Errors'])\n",
    "\n",
    "            for j, feat_name in enumerate(independent_variables):\n",
    "                stars = '***' if float(p_values[j]) < 0.001 else '**' if float(p_values[j]) < 0.01 else '*' if float(p_values[j]) < 0.05 else ''\n",
    "                temp[f'{feat_name.replace(\"_\", \" \").capitalize()}'] = f\"{float(coefficients[j]):.2f}{stars} ({float(standard_errors[j]):.2f})\"\n",
    "\n",
    "\n",
    "                if len(independent_variables) ==  3:\n",
    "                    l = 0.3\n",
    "                    j_pos = 2 * l * name2idx[row['Name']] / (len(name2idx) - 1) + j - l\n",
    "                    if j == 0:\n",
    "                        ax[temperature2idx[row['Temperature']]].errorbar(float(coefficients[j]), j_pos, xerr=float(standard_errors[j]), fmt='o', color=palette[name2idx[row['Name']]], capsize=5, label=row['Name'])\n",
    "                    else:\n",
    "                        ax[temperature2idx[row['Temperature']]].errorbar(float(coefficients[j]), j_pos, xerr=float(standard_errors[j]), fmt='o', color=palette[name2idx[row['Name']]], capsize=5)\n",
    "\n",
    "            if len(independent_variables) ==  3:\n",
    "\n",
    "                ax[temperature2idx[row['Temperature']]].set_ylim(-1, 3)\n",
    "                ax[temperature2idx[row['Temperature']]].set_yticks(range(len(independent_variables)))\n",
    "                ax[temperature2idx[row['Temperature']]].set_yticklabels([variables2latex[feat_name] for feat_name in independent_variables])\n",
    "                ax[temperature2idx[row['Temperature']]].set_title(f'Temperature = {row[\"Temperature\"]}')\n",
    "                ax[temperature2idx[row['Temperature']]].set_xlim(-0.25, 2.75)\n",
    "                ax[temperature2idx[row['Temperature']]].plot([0, 0], [-1, 3], color='#7f8c8d', linewidth=1.0, linestyle='--')\n",
    "\n",
    "    for i in range(len(temperature2idx)):\n",
    "        ax[i].spines[['right', 'top']].set_visible(False)\n",
    "        if i == len(temperature2idx) - 1:\n",
    "            ax[i].legend(fontsize=0.5*SMALL_SIZE, loc='lower right')\n",
    "          \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(outfile, dpi=300, bbox_inches='tight')\n",
    "            \n",
    "\n",
    "def prepare_discrete_choice_model(results, candidates, bias=True, feature_names=['degree'], log_transform=True, exclude_log=[]):\n",
    "\n",
    "    choice_sets = []\n",
    "    choices = []\n",
    "\n",
    "    chosen_set = set()\n",
    "    dropped_set = set()\n",
    "    df_records = []\n",
    "\n",
    "    for result in results:\n",
    "        num_choices = len(result)\n",
    "        choice = np.ones((len(feature_names) + int(bias), num_choices))\n",
    "        for i, r in enumerate(result):\n",
    "            for j, feat_name in enumerate(feature_names):\n",
    "                if log_transform and feat_name not in exclude_log:\n",
    "                    choice[j, i] = np.log(r['similarity'][feat_name] + 1)\n",
    "                else:\n",
    "                    choice[j, i] = r['similarity'][feat_name]\n",
    "\n",
    "            chosen_set |= {r['name']}\n",
    "            dropped_set |= {r['name']} if r['dropped'] else set()\n",
    "\n",
    "        choices.append(choice)\n",
    "\n",
    "    for candidate in candidates:\n",
    "        choice_set = np.ones((len(feature_names) + int(bias), len(candidate)))\n",
    "\n",
    "        for i, c in enumerate(candidate):\n",
    "            for j, feat_name in enumerate(feature_names):\n",
    "                if log_transform and feat_name not in exclude_log:\n",
    "                    choice_set[j, i] = np.log(c['similarity'][feat_name] + 1)\n",
    "                else:\n",
    "                    choice_set[j, i] = c['similarity'][feat_name]\n",
    "\n",
    "            c['similarity']['y'] = 1 if c['name'] in chosen_set else 0\n",
    "            c['similarity']['dropped'] = 1 if c['name'] in dropped_set else 0\n",
    "            df_records.append(c['similarity'])\n",
    "\n",
    "        choice_sets.append(choice_set)\n",
    "        \n",
    "    df = pd.DataFrame.from_records(df_records)\n",
    "\n",
    "    if log_transform:\n",
    "        for feat_name in feature_names:\n",
    "            if feat_name not in exclude_log:\n",
    "                df[feat_name] = np.log(df[feat_name] + 1)\n",
    "\n",
    "    return choices, choice_sets, df\n",
    "\n",
    "def plot_choices(results, candidates, bias=True, feature_names=['degree', 'common_attributes', 'common_neighbors'], log_transform=True, exclude_log=[]):\n",
    "\n",
    "    _, _, df = prepare_discrete_choice_model(results, candidates, bias=bias, feature_names=feature_names, log_transform=log_transform, exclude_log=exclude_log)\n",
    "\n",
    "    fig, ax = plt.subplots(3, len(feature_names), figsize=(5 * len(feature_names), 15))\n",
    "\n",
    "\n",
    "    X = df[feature_names]\n",
    "    if bias:\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "    y = df['y']\n",
    "\n",
    "    # fit logistic regression model\n",
    "    model = sm.Logit(y, X)\n",
    "\n",
    "    results = model.fit()\n",
    "\n",
    "    param_name = {\n",
    "        'common_attributes' : '$\\\\hat \\\\theta_{H}$',\n",
    "        'common_neighbors' : '$\\\\hat \\\\theta_{TC}$',\n",
    "        'degree' : '$\\\\hat \\\\theta_{PA}$',\n",
    "    }\n",
    "\n",
    "    for i, feat_name in enumerate(feature_names):\n",
    "        df[df['y'] == 1][feat_name].hist(ax=ax[0, i], color='#d35400', density=True)\n",
    "        ax[0, i].set_title(f'{feat_name} (log)')\n",
    "        stars = '(***)' if results.pvalues[feat_name] < 0.001 else '(**)' if results.pvalues[feat_name] < 0.01 else '(*)' if results.pvalues[feat_name] < 0.05 else ''\n",
    "\n",
    "        # write logit coefficient and p-value in the plot\n",
    "        # ax[0, i].text(1, 0.9, f'{param_name[feat_name]} = {results.params[feat_name]:.2f} {stars}', horizontalalignment='center', verticalalignment='center', transform=ax[0, i].transAxes, fontsize=SMALL_SIZE)\n",
    "\n",
    "        df[df['y'] == 0][feat_name].hist(ax=ax[1, i], color='#34495e', density=True)\n",
    "\n",
    "        df[feat_name].hist(ax=ax[2, i], color='#2980b9', density=True)\n",
    "\n",
    "    # fig.suptitle('Selected nodes feature distribution')\n",
    "    ax[0, 0].set_ylabel('Selected')\n",
    "    ax[1, 0].set_ylabel('Not Selected')\n",
    "    ax[2, 0].set_ylabel('All')\n",
    "\n",
    "    # remove grid and despine\n",
    "    for i in range(3):\n",
    "        for j in range(len(feature_names)):\n",
    "            ax[i, j].grid(False)\n",
    "            ax[i, j].spines[['right', 'top']].set_visible(False)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig('choices.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fit_discrete_choice_model(results, candidates, bias=True, feature_names=['degree', 'common_attributes', 'common_neighbors'], log_transform=True, exclude_log=[]):\n",
    "\n",
    "    choices, choice_sets, _ = prepare_discrete_choice_model(results, candidates, bias=bias, feature_names=feature_names, log_transform=log_transform, exclude_log=exclude_log)\n",
    "\n",
    "    theta = np.zeros(len(feature_names) + int(bias))\n",
    "\n",
    "    ll = lambda x: -discrete_choice_model_log_likelihood(x, choice_sets, choices)\n",
    "\n",
    "    res = scipy.optimize.minimize(ll, x0=theta, method='L-BFGS-B')\n",
    "\n",
    "    theta = res.x\n",
    "\n",
    "    log_likelihood = -res.fun\n",
    "\n",
    "    standard_errors = (res.hess_inv.todense().diagonal() / len(choices)) ** 0.5\n",
    "\n",
    "    relative_probabilities, log_Z_mean = discrete_choice_model_relative_probability(theta, choice_sets, choices)\n",
    "\n",
    "    return theta, relative_probabilities, log_Z_mean, log_likelihood, standard_errors, choices, choice_sets\n",
    "\n",
    "\n",
    "def discrete_choice_model_log_likelihood(theta, choice_sets, choices):\n",
    "\n",
    "    log_likelihood = 0\n",
    "\n",
    "    for choice_set, choice in zip(choice_sets, choices):\n",
    "        choice_set_utilities = np.dot(theta, choice_set)\n",
    "        Z = np.sum(np.exp(choice_set_utilities))\n",
    "\n",
    "        if Z == 0:\n",
    "            continue\n",
    "\n",
    "        num_choices = choice.shape[1]\n",
    "\n",
    "        for i in range(num_choices):\n",
    "            choice_utility = np.dot(theta, choice[:, i])\n",
    "            log_likelihood += choice_utility - np.log(Z)\n",
    "\n",
    "    return log_likelihood\n",
    "\n",
    "def discrete_choice_model_relative_probability(theta, choice_sets, choices):\n",
    "    probabilities = []\n",
    "    Zs = []\n",
    "    num_errors = 0\n",
    "    for choice_set, choice in zip(choice_sets, choices):\n",
    "        choice_set_utilities = np.dot(theta, choice_set)\n",
    "\n",
    "        Z = np.sum(np.exp(choice_set_utilities))\n",
    "\n",
    "        if Z == 0:\n",
    "            num_errors += 1\n",
    "            continue\n",
    "\n",
    "        Zs.append(np.log(Z))\n",
    "\n",
    "        num_choices = choice.shape[1]\n",
    "\n",
    "        for i in range(num_choices):\n",
    "            choice_utility = np.dot(theta, choice[:, i])\n",
    "            probabilities.append(np.exp(choice_utility) / (Z))\n",
    "\n",
    "    print(f'Number of errors: {num_errors}')\n",
    "\n",
    "    return np.array(probabilities), np.mean(Zs)\n",
    "\n",
    "def modularity_change(filenames, subgraph=False):\n",
    "\n",
    "    for filename in filenames:\n",
    "\n",
    "        with open(filename) as f:\n",
    "            lines = f.read().splitlines()\n",
    "\n",
    "        for line in lines:\n",
    "            temp = json.loads(line)\n",
    "\n",
    "            G0 = nx.from_dict_of_dicts(temp['graphs'][0])\n",
    "            G1 = nx.from_dict_of_dicts(temp['graphs'][-1])\n",
    "\n",
    "            if subgraph:\n",
    "                H = nx.difference(G1, G0)\n",
    "                H.remove_nodes_from(list(nx.isolates(H)))                \n",
    "                G0 = nx.subgraph(G0, H.nodes())\n",
    "                G1 = nx.subgraph(G1, H.nodes())\n",
    "\n",
    "            modularities0 = []\n",
    "            modularities1 = []\n",
    "\n",
    "            for seed in range(10):\n",
    "                communities0 = nx.community.louvain_communities(G0, seed=seed)\n",
    "                modularities0.append(nx.community.modularity(G0, communities0))\n",
    "\n",
    "                communities1 = nx.community.louvain_communities(G1, seed=seed)\n",
    "                modularities1.append(nx.community.modularity(G1, communities1))\n",
    "\n",
    "\n",
    "            t, p = stats.ttest_ind(modularities0, modularities1, equal_var=False, alternative='less')\n",
    "\n",
    "            print(f'{temp[\"name\"]}, {temp[\"temperature\"]}, T-test: {t}, p-value: {p}')\n",
    "\n",
    "def lcc(G):\n",
    "    Gcc = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "    return G.subgraph(Gcc[0])\n",
    "\n",
    "def small_worldness(filenames, name, dataloader_fn, subgraph=False):\n",
    "\n",
    "    networks = dataloader_fn()\n",
    "\n",
    "    G_initial = networks[list(networks.keys())[0]]\n",
    "\n",
    "    # LCC subgraph\n",
    "    G_initial = lcc(G_initial)\n",
    "\n",
    "    average_shortest_path_length_initial = nx.average_shortest_path_length(G_initial)\n",
    "    clustering_coefficient_initial = nx.average_clustering(G_initial)\n",
    "\n",
    "    for filename in filenames:\n",
    "\n",
    "            with open(filename) as f:\n",
    "                lines = f.read().splitlines()\n",
    "\n",
    "            for line in lines:\n",
    "                temp = json.loads(line)\n",
    "\n",
    "                G0 = nx.from_dict_of_dicts(temp['graphs'][0])\n",
    "                G1 = nx.from_dict_of_dicts(temp['graphs'][-1])\n",
    "\n",
    "                G1 = lcc(G1)\n",
    "                # if subgraph:\n",
    "                #     H = nx.difference(G1, G0)\n",
    "                #     H.remove_nodes_from(list(nx.isolates(H)))                \n",
    "                #     G0 = nx.subgraph(G0, H.nodes())\n",
    "                #     G1 = nx.subgraph(G1, H.nodes())\n",
    "\n",
    "                average_shortest_path_length = nx.average_shortest_path_length(G1)\n",
    "                clustering_coefficient = nx.average_clustering(G1)\n",
    "\n",
    "                average_shortest_path_length_initial_change = (average_shortest_path_length - average_shortest_path_length_initial) / average_shortest_path_length_initial * 100\n",
    "                clustering_coefficient_initial_change = (clustering_coefficient - clustering_coefficient_initial) / clustering_coefficient_initial * 100\n",
    "\n",
    "                print(f'{temp[\"name\"]}, {temp[\"temperature\"]}, Average Shortest Path Length Change: {average_shortest_path_length_initial_change}, Clustering Coefficient Change: {clustering_coefficient_initial_change}')\n",
    "\n",
    "def measure_relative_increase(filenames):\n",
    "\n",
    "    for filename in filenames:\n",
    "\n",
    "        with open(filename) as f:\n",
    "            lines = f.read().splitlines()\n",
    "\n",
    "        data = []\n",
    "\n",
    "        for line in lines:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "        for d in data:\n",
    "            total, count = 0, 0        \n",
    "            for results in d[\"results\"]:\n",
    "                for result in results:\n",
    "                    count += int(result['dropped'])\n",
    "                    total += 1          \n",
    "\n",
    "            accuracy = count / total * 100\n",
    "            random_guess = 100 / d[\"num_choices\"]\n",
    "            relative_increase = (accuracy - random_guess) / random_guess * 100\n",
    "\n",
    "            print(f'{d[\"name\"]}, {d[\"temperature\"]}, {d[\"simulation\"]}, Relative Increase in Accuracy % = {relative_increase}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulations\n",
    "models = ['gpt-3.5-turbo', 'meta/meta-llama-3-70b-instruct', 'gpt-4o-mini', 'claude-3-5-sonnet-20240620', 'gpt-4-1106-preview']\n",
    "\n",
    "datasets = ['Caltech36', 'Swarthmore42', 'UChicago30', 'andorra', 'mobiled']\n",
    "temperatures = [0.5]\n",
    "\n",
    "num_egonets = -1\n",
    "egonets_radius = -1\n",
    "sample_egonets = False\n",
    "\n",
    "for model in models:\n",
    "    print(model)\n",
    "    for name in datasets:\n",
    "        for i, temperature in enumerate(temperatures, 1):\n",
    "            outfile = f'outputs/combined_model_{name.lower()}_{i}+{model.replace(\"/\", \"-\")}.jsonl'\n",
    "            if name == 'andorra':\n",
    "                dataloader_fn = lambda: dataloader.load_andorra(input_dir='datasets/andorra')\n",
    "                run_network_formation_experiment(name=name, num_nodes_samples=1000, num_simulations=1, outfile=outfile, temperatures=[temperature], method='llm', num_choices=1, num_samples=5, model=model, dataloader_fn=dataloader_fn)\n",
    "            elif name == 'mobiled':\n",
    "                dataloader_fn = lambda: dataloader.load_mobiled(input_dir='datasets/mobiled')\n",
    "                run_network_formation_experiment(name=name, num_nodes_samples=1000, num_simulations=1, outfile=outfile, temperatures=[temperature], method='llm', num_choices=1, num_samples=5, model=model, dataloader_fn=dataloader_fn)    \n",
    "            else:\n",
    "                dataloader_fn = lambda: dataloader.load_facebook100(input_dir='datasets/facebook100', name=name, num_egonets=num_egonets, egonets_radius=egonets_radius, sample_egonets=sample_egonets)\n",
    "                run_network_formation_experiment(name=name, num_nodes_samples=2000, num_simulations=1, outfile=outfile, temperatures=[temperature], method='llm', num_choices=1, num_samples=15, model=model, dataloader_fn=dataloader_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and format regression tables\n",
    "for model in models:\n",
    "    for name in datasets:\n",
    "        for i in range(1, 1 + len(temperatures)):\n",
    "            generate_regression_table(f'outputs/combined_model_{name.lower()}_{i}+{model.replace(\"/\", \"-\")}.jsonl', f'tables/combined_model_{name.lower()}_{i}+{model.replace(\"/\", \"-\")}.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export regression tables to LaTeX\n",
    "for model in models:\n",
    "    for name in datasets:\n",
    "        for full in [True, False]:\n",
    "            pretty_print_regression_table([f'tables/combined_model_{name.lower()}_{i}+{model.replace(\"/\", \"-\")}.xlsx' for i in range(1, 1 + len(temperatures))], f'tables/combined_model_{name.lower()}{\"_full\" if full else \"\"}+{model.replace(\"/\", \"-\")}.tex', full=full)\n",
    "\n",
    "    plot_coefficients([f'tables/combined_model_{name.lower()}_{i}+{model.replace(\"/\", \"-\")}.xlsx' for i in range(1, 1 + len(temperatures)) for name in datasets], f'figures/combined_model_coefficients+{model.replace(\"/\", \"-\")}.png')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
