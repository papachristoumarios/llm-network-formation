{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import scipy.stats as stats\n",
    "\n",
    "import seaborn as sns\n",
    "import json\n",
    "import openai\n",
    "import random\n",
    "import os\n",
    "import collections\n",
    "import itertools\n",
    "import copy\n",
    "import dataloader\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "import netgraph\n",
    "import scipy\n",
    "import ast\n",
    "from utils import get_response, summarize_reasons\n",
    "\n",
    "MEDIUM_SIZE = 28\n",
    "SMALL_SIZE = 0.85 * MEDIUM_SIZE\n",
    "BIGGER_SIZE = 1.5 * MEDIUM_SIZE\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "# gpt-4-1106-preview\n",
    "\n",
    "# def summarize_reasons(filenames, name, n_samples=50, n_categories=4, n_resamples=5, categories=None, model='gpt-4-1106-preview'):\n",
    "#     random.seed(1)\n",
    "#     np.random.seed(1)\n",
    "\n",
    "#     reason_list = collections.defaultdict(list)\n",
    "\n",
    "#     all_reasons = []\n",
    "\n",
    "#     for filename in filenames:\n",
    "#         data = []\n",
    "\n",
    "#         with open(filename) as f:\n",
    "#             lines = f.read().splitlines()\n",
    "\n",
    "\n",
    "#         for line in lines:\n",
    "\n",
    "#             data.append(json.loads(line))\n",
    "\n",
    "\n",
    "#         for d in data:        \n",
    "#             for results in d[\"results\"]:\n",
    "#                 for result in results:\n",
    "#                     if result and 'reason' in result.keys():\n",
    "#                         reason_list[d['temperature']].append(result['reason'])\n",
    "#                         all_reasons.append(result['reason'])\n",
    "\n",
    "#     if categories is None:\n",
    "#         categorization_prompt = f\"\"\"\n",
    "#         # Task\n",
    "\n",
    "#         You are given a list of reasons and your task to find {n_categories} categories that best describe the reasons.\n",
    "\n",
    "#         # Input\n",
    "\n",
    "#         The input is a list of reasons. The list is given below after chevrons:\n",
    "#         <REASONS>\n",
    "#         {json.dumps(random.sample(all_reasons, len(reason_list) * n_samples))}\n",
    "#         </REASONS>\n",
    "\n",
    "#         # Output\n",
    "\n",
    "#         The output should be given in JSON format with the following structure:\n",
    "\n",
    "#         [\n",
    "#             {{\n",
    "#                 \"category\" : category,\n",
    "#                 \"description\" : short description of the category\n",
    "#             }}, ...\n",
    "#         ]\n",
    "\n",
    "#         # Notes\n",
    "#         * The names of the categories must be descriptive and mutually exclusive.\n",
    "\n",
    "#         ```json\n",
    "#         \"\"\"\n",
    "\n",
    "#         for _ in range(10):\n",
    "#             try:\n",
    "#                 ans = get_response(categorization_prompt, temperature=0, system_prompt=\"You are a helpful assistant\", model=model)\n",
    "#                 categories = json.loads(ans.lstrip('```json').rstrip('```'))\n",
    "#                 print(categories)\n",
    "#                 break\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(e)\n",
    "        \n",
    "#     fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "#     category_list = [c['category'] for c in categories]\n",
    "\n",
    "#     records = []\n",
    "\n",
    "#     for i, (k, v) in enumerate(reason_list.items()):\n",
    "#         print('Temperature', k)\n",
    "#         if len(v) <= n_samples:\n",
    "#             n_resamples = 1\n",
    "\n",
    "#         for r in range(n_resamples):\n",
    "#             prompt = f\"\"\"\n",
    "#             # Task\n",
    "#             You are given a list of reasons and your task is to classify them into categories.\n",
    "\n",
    "#             # Input\n",
    "#             The input is a list of reasons. The list is given below after chevrons:\n",
    "#             <REASONS>\n",
    "#             {json.dumps(random.sample(v, n_samples), indent=4)}\n",
    "#             </REASONS>\n",
    "\n",
    "#             ## Categories\n",
    "#             The names of the categories are given below after chevrons:\n",
    "#             <CATEGORIES>\n",
    "#             {json.dumps(categories, indent=4)}\n",
    "#             </CATEGORIES>\n",
    "\n",
    "#             Each reason must be assigned to exactly one of the categories.\n",
    "            \n",
    "#             # Output\n",
    "#             The output should be given as a list of JSON objects with the following structure:\n",
    "\n",
    "#             [\n",
    "#                 {{\n",
    "#                         \"reason\" : reason,\n",
    "#                         \"category\" : category name\n",
    "#                 }}, ...\n",
    "#             ]\n",
    "\n",
    "#             ```json\n",
    "#             \"\"\"\n",
    "\n",
    "#             for _ in range(10):\n",
    "#                 try:\n",
    "#                     ans = get_response(prompt, temperature=0, system_prompt=\"You are a helpful assistant\", model=model)\n",
    "\n",
    "#                     result =  json.loads(ans.lstrip('```json').rstrip('```'))\n",
    "\n",
    "\n",
    "#                     assert(isinstance(result, list))\n",
    "\n",
    "#                     reason_types = collections.defaultdict(float)\n",
    "\n",
    "#                     total = 0\n",
    "\n",
    "#                     for reason in result:\n",
    "#                         if reason['category'] in category_list:\n",
    "#                             reason_types[reason['category']] += 1\n",
    "#                             total += 1\n",
    "\n",
    "#                     for key, val in reason_types.items():\n",
    "#                         reason_types[key] = val / total * 100\n",
    "\n",
    "                   \n",
    "#                     break\n",
    "#                 except Exception as e:\n",
    "#                     print(e)\n",
    "\n",
    "#             for key, val in reason_types.items():\n",
    "#                 records.append({\n",
    "#                     'Temperature' : k,\n",
    "#                     'Category' : key,\n",
    "#                     'Frequency' : val,\n",
    "#                     'Resample' : r\n",
    "#                 })\n",
    "            \n",
    "#     df = pd.DataFrame.from_records(records)\n",
    "\n",
    "#     fig.suptitle(f'Reasoning for {name}', fontsize=MEDIUM_SIZE)\n",
    "\n",
    "#     sns.barplot(data=df, x='Category', y='Frequency', hue='Temperature', ax=ax, palette='Set2', hue_order=sorted(reason_list.keys()), order=sorted(category_list))\n",
    "\n",
    "#     plt.legend(fontsize=0.6*SMALL_SIZE, title='Temperature', loc='upper left')\n",
    "\n",
    "#     plt.xticks(rotation=0, fontsize=0.6*SMALL_SIZE)\n",
    "\n",
    "#     # T-test\n",
    "\n",
    "#     for k in sorted(reason_list.keys()):\n",
    "#         for category1, category2 in itertools.combinations(category_list, 2):\n",
    "#             print(f'Temperature: {k}, Category 1: {category1}, Category 2: {category2}')\n",
    "#             print(stats.ttest_ind(df.query(f'Temperature == {k} and Category == \"{category1}\"')['Frequency'], df.query(f'Temperature == {k} and Category == \"{category2}\"')['Frequency'], equal_var=False, alternative='greater'))\n",
    "\n",
    "\n",
    "#     fig.tight_layout()\n",
    "\n",
    "#     fig.savefig(f'figures/combined_model/{name}_reasons.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "def network_growth(G0, temperature, num_choices=1, method='llm', num_samples=-1, num_nodes_samples=-1, model='gpt-4-1106-preview'):\n",
    "    # Set seed\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)    \n",
    "\n",
    "    # Copy the ground truth graph\n",
    "    G = G0.copy()\n",
    "   \n",
    "    Gs = [G.copy()]\n",
    "\n",
    "    profiles = nx.get_node_attributes(G, 'features')\n",
    "\n",
    "    # Edges to drop\n",
    "    dropped_edges = []\n",
    "\n",
    "    if num_nodes_samples > 0:\n",
    "        nodes = random.sample(G.nodes(), min(len(G), num_nodes_samples))\n",
    "    else:\n",
    "        nodes = G.nodes()\n",
    "\n",
    "    # Drop one neighbor for each node\n",
    "    for v in nodes:\n",
    "        dropped_v_edges = []\n",
    "        for _ in range(num_choices):\n",
    "            if len(list(G.neighbors(v))) > 0:\n",
    "                \n",
    "                while True:\n",
    "                    u = random.choice(list(G.neighbors(v)))\n",
    "                    if (v, u) not in dropped_edges:\n",
    "                        dropped_v_edges.append((v, u))\n",
    "                        G.remove_edge(v, u)\n",
    "                        break\n",
    "\n",
    "        dropped_edges.append(dropped_v_edges)\n",
    "\n",
    "    Gs = [G.copy()]\n",
    "    results = []\n",
    "    candidates = []\n",
    "\n",
    "\n",
    "    for i, t in enumerate(nodes):\n",
    "\n",
    "        if method == 'llm':\n",
    "            result, candidate = select_neighbor(G, t, profiles, temperature, num_choices=len(dropped_edges[i]), dropped_nodes=[u for (_, u) in dropped_edges[i]], num_samples=num_samples, model=model)\n",
    "\n",
    "            if result:\n",
    "                for r in result:\n",
    "                    v = r['name']\n",
    "                    r['edge'] = (t, v)\n",
    "                    G.add_edge(t, v, similarity=r['similarity'])\n",
    "                results.append(result)\n",
    "\n",
    "            candidates.append(candidate)\n",
    "        elif method in ['random', 'homophilous', 'heterophilous', 'ground_truth']:\n",
    "            if num_samples > 0:\n",
    "                choice_set = random.sample([v for v in G.nodes() if v != t], num_samples)\n",
    "            else:\n",
    "                choice_set = [v for v in G.nodes() if v != t]\n",
    "\n",
    "            if method == 'random':\n",
    "                new_nodes = random.sample(choice_set, len(dropped_edges[i]))\n",
    "            elif method == 'homophilous':\n",
    "                new_nodes = list(sorted(choice_set, key=lambda v: measure_similarity(profiles[t], profiles[v])['common_attributes'], reverse=True))[:len(dropped_edges[i])]\n",
    "            elif method == 'heterophilous':\n",
    "                new_nodes = list(sorted(choice_set, key=lambda v: measure_similarity(profiles[t], profiles[v])['common_attributes']))[:len(dropped_edges[i])]\n",
    "            elif method == 'ground_truth':\n",
    "                new_nodes = [e[1] for e in dropped_edges[i]]\n",
    "\n",
    "            result = []\n",
    "\n",
    "            for v in new_nodes:\n",
    "                print(f'Node: {t}, Link: {v}')\n",
    "                similarity = measure_similarity(profiles[t], profiles[v])\n",
    "                G.add_edge(t, v, similarity=similarity, weight=similarity['common_attributes'])\n",
    "            \n",
    "                result.append({'name' : v, 'similarity' : similarity, 'reason' : method})\n",
    "\n",
    "            candidate = []\n",
    "\n",
    "            for v in choice_set:\n",
    "                similarity = measure_similarity(profiles[t], profiles[v])\n",
    "                candidate.append({'name' : v, 'similarity' : similarity, 'reason' : method})\n",
    "\n",
    "            candidates.append(candidate)\n",
    "            results.append(result)\n",
    "\n",
    "        Gs.append(G.copy())\n",
    "\n",
    "    return Gs, results, candidates\n",
    "\n",
    "def fit_dcm(results):\n",
    "\n",
    "    similarities = [r['similarity'] for result in results for r in result]\n",
    "    similarities_df = pd.DataFrame.from_records(similarities)\n",
    "    similarities_df = sm.add_constant(similarities_df)\n",
    "\n",
    "    outcomes = np.array([r['edge'][1] for result in results for r in result])\n",
    "\n",
    "    print(similarities_df)\n",
    "\n",
    "    mnl_model = sm.MNLogit(outcomes, similarities_df)\n",
    "    mnl_results = mnl_model.fit()\n",
    "\n",
    "    print(mnl_results.summary())\n",
    "\n",
    "    return mnl_results\n",
    "\n",
    "def measure_similarity(profile1, profile2):\n",
    "    similarity = {\n",
    "        'common_attributes' : 0,\n",
    "        'common_neighbors' : len(set(profile1['neighbors']) & set(profile2['neighbors'])),\n",
    "        'degree' : profile2['degree'],\n",
    "    }\n",
    "\n",
    "    for k in profile1.keys():\n",
    "        if k != 'name' and k != 'neighbors' and k in profile2.keys():\n",
    "            if profile1[k] == profile2[k]:\n",
    "                similarity['common_attributes'] += 1\n",
    "        \n",
    "    return similarity\n",
    "\n",
    "\n",
    "def select_neighbor(G, t, profiles, temperature, num_choices=1, num_samples=-1, dropped_nodes=[], model='gpt-4-1106-preview'):\n",
    "\n",
    "    if num_samples > 0:\n",
    "        choice_set = random.sample([v for v in G.nodes() if v != t and v not in G.neighbors(t)], max(0, num_samples - len(dropped_nodes))) + dropped_nodes\n",
    "    else:\n",
    "        choice_set = [v for v in G.nodes() if v != t and v not in G.neighbors(t)]\n",
    "\n",
    "    candidate_profiles = []\n",
    "\n",
    "    for v in choice_set + [t]:\n",
    "        profiles[v]['neighbors'] = list(G.neighbors(v))\n",
    "        profiles[v]['degree'] = len(profiles[v]['neighbors']) \n",
    "        profiles[v]['name'] = v                 \n",
    "        candidate_profiles.append(profiles[v])\n",
    "\n",
    "    random.shuffle(candidate_profiles)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    # Task\n",
    "    Your task is to select a set of people to be friends with.\n",
    "\n",
    "    # Profile\n",
    "    Your profile is given below after chevrons:\n",
    "    <PROFILE>\n",
    "    {json.dumps(profiles[t])}\n",
    "    </PROFILE>\n",
    "\n",
    "    # Candidate Profiles\n",
    "    The cadidate profiles to be friends with are given below after chevrons:\n",
    "\n",
    "    <PROFILES>\n",
    "    {json.dumps(candidate_profiles)}\n",
    "    </PROFILES>\n",
    "\n",
    "    # Output\n",
    "    The output should be given a list of JSON objects with the following structure\n",
    "\n",
    "    [\n",
    "        {{\n",
    "            \"name\" : name of the person you selected,\n",
    "            \"reason\" : reason for selecting the person\n",
    "        }}, ...\n",
    "    ]\n",
    "\n",
    "    # Notes\n",
    "    * The output must be a list of JSON objects ranked in the order of preference.\n",
    "    * You can make at most {num_choices} selection{'s' if num_choices > 1 else ''}.\n",
    "    \n",
    "    ```json\n",
    "    \"\"\"   \n",
    "\n",
    "    for _ in range(10):\n",
    "        try:\n",
    "            ans = get_response(prompt, temperature=temperature, model=model)\n",
    "            try:\n",
    "                results = json.loads(ans.split('```')[0])\n",
    "            except:\n",
    "                results = json.loads(ans.split('```json')[1].split('```')[0])\n",
    "\n",
    "            filtered_results = []\n",
    "            for result in results:\n",
    "                v = result['name']\n",
    "                if v in G.nodes():\n",
    "                    result['similarity'] = measure_similarity(profiles[t], profiles[v])\n",
    "                    filtered_results.append(result)\n",
    "\n",
    "                    result['dropped'] = v in dropped_nodes\n",
    "\n",
    "            print(f'Node: {t}, Links: {filtered_results}')\n",
    "\n",
    "            candidates = []\n",
    "\n",
    "            for candidate_profile in candidate_profiles:\n",
    "                similarity = measure_similarity(profiles[t], candidate_profile)\n",
    "                candidates.append({'name' : candidate_profile['name'], 'similarity' : similarity})\n",
    "\n",
    "            return filtered_results, candidates\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    return [], []\n",
    "\n",
    "\n",
    "def run_network_formation_experiment(name, num_simulations, outfile, temperatures, method, num_choices, num_samples, num_nodes_samples, model, dataloader_fn):\n",
    "    networks = dataloader_fn()\n",
    "    \n",
    "    saved_scenarios = set()\n",
    "\n",
    "    if os.path.exists(outfile):\n",
    "        with open(outfile) as f:\n",
    "            lines = f.read().splitlines()\n",
    "\n",
    "            for line in lines:\n",
    "                scenario = json.loads(line)\n",
    "                saved_scenarios.add((scenario['name'], scenario['ego'], scenario['simulation'], scenario['temperature'], scenario['num_samples'], scenario['num_choices']))\n",
    "\n",
    "        exit()\n",
    "\n",
    "    f = open(outfile, 'a+')\n",
    "\n",
    "    for ego, G0 in networks.items():\n",
    "        for i in range(num_simulations):\n",
    "            for temperature in temperatures:\n",
    "                if (name, ego, i, temperature, num_samples, num_choices) in saved_scenarios:\n",
    "                    print(f'Skipping simulation for name={name}, ego={ego}, i={i}, temperature={temperature}, num_choices={num_choices}, num_samples={num_samples}, method={method}')\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f'Running simulation for name={name}, ego={ego}, i={i}, temperature={temperature}, num_choices={num_choices}, num_samples={num_samples}, method={method}')\n",
    "\n",
    "                    Gs, results, candidates = network_growth(G0, temperature=temperature, method=method, num_choices=num_choices, num_samples=num_samples, num_nodes_samples=num_nodes_samples, model=model)\n",
    "\n",
    "                    temp = {\n",
    "                        'name' : name,\n",
    "                        'ego' : ego,\n",
    "                        'temperature' : temperature,\n",
    "                        'simulation' : i,\n",
    "                        'num_choices' : num_choices,\n",
    "                        'num_samples' : num_samples,\n",
    "                        'graphs' : [nx.to_dict_of_dicts(G) for G in Gs],\n",
    "                        'results' : results,\n",
    "                        'candidates' : candidates,\n",
    "                        'model' : model\n",
    "                    }    \n",
    "\n",
    "                    f.write(json.dumps(temp) + '\\n')            \n",
    "\n",
    "                if method != 'llm':\n",
    "                    break\n",
    "\n",
    "    f.close()\n",
    "\n",
    "def draw_graph(G, ax, communities=None, palette=None):\n",
    "\n",
    "    pos = nx.spring_layout(G)\n",
    "\n",
    "    netgraph.Graph(G, node_layout=pos, node_color='#d35400', node_size=2.5, edge_color='#34495e', edge_width=1, ax=ax)\n",
    "\n",
    "    ax.set_axis_off()\n",
    "\n",
    "\n",
    "def generate_regression_table(filename, outfile):\n",
    "\n",
    "    palette = ['#d35400', '#34495e', '#2980b9', '#e67e22', '#f1c40f', '#7f8c8d', '#27ae60', '#16a085', '#bdc3c7', '#1abc9c', '#2ecc71', '#3498db', '#9b59b6', '#8e44ad', '#ecf0f1']\n",
    "\n",
    "    with open(filename) as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for line in lines:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "    feature_names = ['degree', 'common_attributes', 'common_neighbors']\n",
    "\n",
    "    regression_table_df = []\n",
    "    \n",
    "    names = set([d['name'] for d in data])\n",
    "\n",
    "\n",
    "    for d in data:\n",
    "        # Gs = []\n",
    "        # for graph in d['graphs']:\n",
    "        #     G = nx.from_dict_of_dicts(graph)\n",
    "        #     Gs.append(G)\n",
    "\n",
    "        log_likelihoods = {}\n",
    "\n",
    "        for num_features in range(len(feature_names) + 1):\n",
    "            for feature_combination in itertools.combinations(feature_names, num_features):\n",
    "                feature_combination = list(feature_combination)\n",
    "                theta, relative_probabilities, log_Z_mean, log_likelihood, standard_errors, choices, choice_sets = fit_discrete_choice_model(d['results'], d['candidates'], feature_names=feature_combination, bias=True)\n",
    "                            \n",
    "                temp = {\n",
    "                    'Name' : d[\"name\"],\n",
    "                    'Ego' : d[\"ego\"],\n",
    "                    'Temperature' : d[\"temperature\"],\n",
    "                    'Simulation' : d[\"simulation\"],\n",
    "                    'Number of Choices' : d[\"num_choices\"],\n",
    "                    'Number of Samples' : d[\"num_samples\"],\n",
    "                    'Independent Variable' : feature_combination,\n",
    "                    'Coefficients' : theta[:-1].tolist(),\n",
    "                    'Standard Errors' : standard_errors[:-1].tolist(),\n",
    "                    'Log Likelihood' : log_likelihood,\n",
    "                }\n",
    "\n",
    "                log_likelihoods[tuple(sorted(feature_combination))] = log_likelihood\n",
    "                p_values = np.array([1 - stats.chi2.cdf(2 * (log_likelihood - log_likelihoods[tuple(sorted(feature_combination[:i] + feature_combination[i + 1:]))]), 1) for i in range(len(feature_combination))])\n",
    "\n",
    "                temp['P-values'] = p_values.tolist()\n",
    "\n",
    "                regression_table_df.append(temp)\n",
    "\n",
    "                choices = np.array(choices)\n",
    "\n",
    "                utilities = np.ones(len(relative_probabilities))\n",
    "                utilities_upper = np.ones(len(relative_probabilities))\n",
    "                utilities_lower = np.ones(len(relative_probabilities))\n",
    "\n",
    "                feat_linspaces = []\n",
    "\n",
    "                for i, feat_name in enumerate(feature_combination):\n",
    "                    feat_linspace = np.linspace(np.min(choices[:, i, 0]), np.max(choices[:, i, 0]), len(relative_probabilities))\n",
    "                    \n",
    "\n",
    "                    utilities *= np.exp(theta[i] * feat_linspace)\n",
    "                    utilities_upper *= np.exp((theta[i] + standard_errors[i]) * feat_linspace)\n",
    "                    utilities_lower *= np.exp((theta[i] - standard_errors[i]) * feat_linspace)\n",
    "\n",
    "                    feat_linspaces.append(feat_linspace)\n",
    "\n",
    "                utilities *= np.exp(theta[-1])\n",
    "                utilities_upper *= np.exp(theta[-1] + standard_errors[-1])\n",
    "                utilities_lower *= np.exp(theta[-1] - standard_errors[-1])\n",
    "\n",
    "                utilities /= np.exp(log_Z_mean)\n",
    "                utilities_upper /= np.exp(log_Z_mean)\n",
    "                utilities_lower /= np.exp(log_Z_mean)\n",
    "\n",
    "        if num_features == 3:\n",
    "            # Plot relative probabilities\n",
    "            fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "            fig.suptitle(f'Joint MNL Model for {d[\"name\"]} (Temperature = {d[\"temperature\"]})')\n",
    "\n",
    "            ax[0].set_ylabel('relative probability (log)')\n",
    "            \n",
    "            for i, feat_name in enumerate(feature_names):\n",
    "                sns.scatterplot(x=choices[:, i, 0], y=np.log(relative_probabilities), ax=ax[i], color=palette[i], s=10, alpha=0.5)\n",
    "                ax[i].set_xlabel(feat_name.replace('_', ' ') + ' (log)')\n",
    "\n",
    "                ax[i].plot(feat_linspaces[i], np.log(utilities), color=palette[i], linewidth=2)\n",
    "                ax[i].fill_between(feat_linspaces[i], np.log(utilities_lower), np.log(utilities_upper), color=palette[i], alpha=0.2)\n",
    "\n",
    "                ax[i].spines[['right', 'top']].set_visible(False)\n",
    "\n",
    "            fig.tight_layout()\n",
    "\n",
    "            fig.savefig(f'figures/combined_model/{d[\"name\"]}_temperature_{d[\"temperature\"]}_simulation_{d[\"simulation\"]}_relative_probabilities.png')\n",
    "\n",
    "\n",
    "    regression_table_df = pd.DataFrame.from_records(regression_table_df)\n",
    "\n",
    "    regression_table_df.to_excel(outfile)\n",
    "\n",
    "\n",
    "def pretty_print_regression_table(filenames, outfile, full=False):\n",
    "\n",
    "    if isinstance(filenames, str):\n",
    "        filenames = [filenames]\n",
    "\n",
    "    regression_table_df = pd.concat([pd.read_excel(filename) for filename in filenames])\n",
    "\n",
    "    regression_table_df = regression_table_df.query('`Independent Variable` != \"[]\"')\n",
    "\n",
    "    table_rows_df = []\n",
    "\n",
    "    ego_row = True\n",
    "\n",
    "    temperature2idx = {}\n",
    "\n",
    "\n",
    "    for i, row in regression_table_df.iterrows():\n",
    "        temp = {}\n",
    "        if row['Ego'] == -1:\n",
    "            ego_row = False\n",
    "        else:\n",
    "            temp['Ego'] = row['Ego']\n",
    "        temp['Temperature'] = row['Temperature']\n",
    "\n",
    "        if row['Temperature'] not in temperature2idx:\n",
    "            temperature2idx[row['Temperature']] = len(temperature2idx)\n",
    "\n",
    "        independent_variables = ast.literal_eval(row['Independent Variable'])\n",
    "\n",
    "        if (not full and len(independent_variables) ==  3) or full:\n",
    "\n",
    "            p_values = ast.literal_eval(row['P-values'])\n",
    "            coefficients = ast.literal_eval(row['Coefficients'])\n",
    "            standard_errors = ast.literal_eval(row['Standard Errors'])\n",
    "\n",
    "            for j, feat_name in enumerate(independent_variables):\n",
    "                stars = '***' if float(p_values[j]) < 0.001 else '**' if float(p_values[j]) < 0.01 else '*' if float(p_values[j]) < 0.05 else ''\n",
    "                temp[f'{feat_name.replace(\"_\", \" \").capitalize()}'] = f\"{float(coefficients[j]):.2f}{stars} ({float(standard_errors[j]):.2f})\"\n",
    "\n",
    "\n",
    "          \n",
    "\n",
    "            temp['Log Likelihood'] = f\"{row['Log Likelihood']:,.2f}\"\n",
    "            temp['AIC'] = f'{2 * (len(independent_variables) + 1) - 2 * row[\"Log Likelihood\"]:,.2f}'\n",
    "\n",
    "          \n",
    "          \n",
    "            table_rows_df.append(temp)\n",
    "\n",
    "\n",
    "    table_rows_df = pd.DataFrame.from_records(table_rows_df, columns=['Ego'] if ego_row else [] +  ['Temperature', 'Degree', 'Common attributes', 'Common neighbors', 'Log Likelihood', 'AIC'])\n",
    "    table_rows_df = table_rows_df.fillna(' ')\n",
    "\n",
    "    table_rows_df.to_latex(outfile, index=False, escape=True, column_format='lcccccc')\n",
    "\n",
    "\n",
    "def plot_coefficients(filenames, outfile):\n",
    "\n",
    "    palette = ['#d35400', '#34495e', '#2980b9', '#e67e22', '#f1c40f', '#7f8c8d', '#27ae60', '#16a085', '#bdc3c7', '#1abc9c', '#2ecc71', '#3498db', '#9b59b6', '#8e44ad', '#ecf0f1']\n",
    "\n",
    "    if isinstance(filenames, str):\n",
    "        filenames = [filenames]\n",
    "\n",
    "    regression_table_df = pd.concat([pd.read_excel(filename) for filename in filenames])\n",
    "\n",
    "    regression_table_df = regression_table_df.query('`Independent Variable` != \"[]\"')\n",
    "\n",
    "    table_rows_df = []\n",
    "\n",
    "    ego_row = True\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    temperature2idx = {}\n",
    "\n",
    "    name2idx = {}\n",
    "\n",
    "    variables2latex = {\n",
    "        'degree' : '$\\\\hat \\\\theta_{PA}$',\n",
    "        'common_attributes' : '$\\\\hat \\\\theta_{H}$',\n",
    "        'common_neighbors' : '$\\\\hat \\\\theta_{TC}$',\n",
    "    }\n",
    "\n",
    "    for i, row in regression_table_df.iterrows():\n",
    "        if row['Name'] not in name2idx:\n",
    "            name2idx[row['Name']] = len(name2idx)\n",
    "\n",
    "    for i, row in regression_table_df.iterrows():\n",
    "        temp = {}\n",
    "        if row['Ego'] == -1:\n",
    "            ego_row = False\n",
    "        else:\n",
    "            temp['Ego'] = row['Ego']\n",
    "        temp['Temperature'] = row['Temperature']\n",
    "\n",
    "        if row['Temperature'] not in temperature2idx:\n",
    "            temperature2idx[row['Temperature']] = len(temperature2idx)\n",
    "\n",
    "        independent_variables = ast.literal_eval(row['Independent Variable'])\n",
    "\n",
    "        if len(independent_variables) ==  3:\n",
    "\n",
    "            p_values = ast.literal_eval(row['P-values'])\n",
    "            coefficients = ast.literal_eval(row['Coefficients'])\n",
    "            standard_errors = ast.literal_eval(row['Standard Errors'])\n",
    "\n",
    "            for j, feat_name in enumerate(independent_variables):\n",
    "                stars = '***' if float(p_values[j]) < 0.001 else '**' if float(p_values[j]) < 0.01 else '*' if float(p_values[j]) < 0.05 else ''\n",
    "                temp[f'{feat_name.replace(\"_\", \" \").capitalize()}'] = f\"{float(coefficients[j]):.2f}{stars} ({float(standard_errors[j]):.2f})\"\n",
    "\n",
    "\n",
    "                if len(independent_variables) ==  3:\n",
    "                    l = 0.3\n",
    "                    j_pos = 2 * l * name2idx[row['Name']] / (len(name2idx) - 1) + j - l\n",
    "                    if j == 0:\n",
    "                        ax[temperature2idx[row['Temperature']]].errorbar(float(coefficients[j]), j_pos, xerr=float(standard_errors[j]), fmt='o', color=palette[name2idx[row['Name']]], capsize=5, label=row['Name'])\n",
    "                    else:\n",
    "                        ax[temperature2idx[row['Temperature']]].errorbar(float(coefficients[j]), j_pos, xerr=float(standard_errors[j]), fmt='o', color=palette[name2idx[row['Name']]], capsize=5)\n",
    "\n",
    "            if len(independent_variables) ==  3:\n",
    "\n",
    "                ax[temperature2idx[row['Temperature']]].set_ylim(-1, 3)\n",
    "                ax[temperature2idx[row['Temperature']]].set_yticks(range(len(independent_variables)))\n",
    "                ax[temperature2idx[row['Temperature']]].set_yticklabels([variables2latex[feat_name] for feat_name in independent_variables])\n",
    "                ax[temperature2idx[row['Temperature']]].set_title(f'Temperature = {row[\"Temperature\"]}')\n",
    "                ax[temperature2idx[row['Temperature']]].set_xlim(-0.25, 2.75)\n",
    "                ax[temperature2idx[row['Temperature']]].plot([0, 0], [-1, 3], color='#7f8c8d', linewidth=1.0, linestyle='--')\n",
    "\n",
    "    for i in range(len(temperature2idx)):\n",
    "        ax[i].spines[['right', 'top']].set_visible(False)\n",
    "        if i == len(temperature2idx) - 1:\n",
    "            ax[i].legend(fontsize=0.5*SMALL_SIZE, loc='lower right')\n",
    "          \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(outfile, dpi=300, bbox_inches='tight')\n",
    "            \n",
    "\n",
    "def prepare_discrete_choice_model(results, candidates, bias=True, feature_names=['degree'], log_transform=True):\n",
    "\n",
    "    choice_sets = []\n",
    "    choices = []\n",
    "\n",
    "    for result in results:\n",
    "        num_choices = len(result)\n",
    "        choice = np.ones((len(feature_names) + int(bias), num_choices))\n",
    "        for i, r in enumerate(result):\n",
    "            for j, feat_name in enumerate(feature_names):\n",
    "                if log_transform:\n",
    "                    choice[j, i] = np.log(r['similarity'][feat_name] + 1)\n",
    "                else:\n",
    "                    choice[j, i] = r['similarity'][feat_name]\n",
    "\n",
    "        choices.append(choice)\n",
    "\n",
    "        \n",
    "\n",
    "    for candidate in candidates:\n",
    "        choice_set = np.ones((len(feature_names) + int(bias), len(candidate)))\n",
    "\n",
    "        for i, c in enumerate(candidate):\n",
    "            for j, feat_name in enumerate(feature_names):\n",
    "                if log_transform:\n",
    "                    choice_set[j, i] = np.log(c['similarity'][feat_name] + 1)\n",
    "                else:\n",
    "                    choice_set[j, i] = c['similarity'][feat_name]\n",
    "\n",
    "        choice_sets.append(choice_set)\n",
    "    \n",
    "    return choices, choice_sets\n",
    "\n",
    "\n",
    "def fit_discrete_choice_model(results, candidates, bias=True, feature_names=['degree', 'common_attributes', 'common_neighbors'], log_transform=True):\n",
    "\n",
    "    choices, choice_sets = prepare_discrete_choice_model(results, candidates, bias=bias, feature_names=feature_names, log_transform=log_transform)\n",
    "\n",
    "    theta = np.zeros(len(feature_names) + int(bias))\n",
    "\n",
    "    ll = lambda x: -discrete_choice_model_log_likelihood(x, choice_sets, choices)\n",
    "\n",
    "    res = scipy.optimize.minimize(ll, x0=theta, method='L-BFGS-B')\n",
    "\n",
    "    theta = res.x\n",
    "\n",
    "    log_likelihood = -res.fun\n",
    "\n",
    "    standard_errors = res.hess_inv.todense().diagonal() ** 0.5\n",
    "\n",
    "    relative_probabilities, log_Z_mean = discrete_choice_model_relative_probability(theta, choice_sets, choices)\n",
    "\n",
    "    return theta, relative_probabilities, log_Z_mean, log_likelihood, standard_errors, choices, choice_sets\n",
    "\n",
    "\n",
    "def discrete_choice_model_log_likelihood(theta, choice_sets, choices):\n",
    "\n",
    "    log_likelihood = 0\n",
    "\n",
    "    for choice_set, choice in zip(choice_sets, choices):\n",
    "        choice_set_utilities = np.dot(theta, choice_set)\n",
    "        Z = np.sum(np.exp(choice_set_utilities))\n",
    "\n",
    "        if Z == 0:\n",
    "            continue\n",
    "\n",
    "        num_choices = choice.shape[1]\n",
    "\n",
    "        for i in range(num_choices):\n",
    "            choice_utility = np.dot(theta, choice[:, i])\n",
    "            log_likelihood += choice_utility - np.log(Z)\n",
    "\n",
    "    return log_likelihood\n",
    "\n",
    "def discrete_choice_model_relative_probability(theta, choice_sets, choices):\n",
    "    probabilities = []\n",
    "    Zs = []\n",
    "    for choice_set, choice in zip(choice_sets, choices):\n",
    "        choice_set_utilities = np.dot(theta, choice_set)\n",
    "        Z = np.sum(np.exp(choice_set_utilities))\n",
    "\n",
    "        if Z == 0:\n",
    "            continue\n",
    "\n",
    "        Zs.append(np.log(Z))\n",
    "\n",
    "        num_choices = choice.shape[1]\n",
    "\n",
    "        for i in range(num_choices):\n",
    "            choice_utility = np.dot(theta, choice[:, i])\n",
    "            probabilities.append(np.exp(choice_utility) / Z)\n",
    "\n",
    "    return np.array(probabilities), np.mean(Zs)\n",
    "\n",
    "def modularity_change(filenames, subgraph=False):\n",
    "\n",
    "    for filename in filenames:\n",
    "\n",
    "        with open(filename) as f:\n",
    "            lines = f.read().splitlines()\n",
    "\n",
    "        for line in lines:\n",
    "            temp = json.loads(line)\n",
    "\n",
    "            G0 = nx.from_dict_of_dicts(temp['graphs'][0])\n",
    "            G1 = nx.from_dict_of_dicts(temp['graphs'][-1])\n",
    "\n",
    "            if subgraph:\n",
    "                H = nx.difference(G1, G0)\n",
    "                H.remove_nodes_from(list(nx.isolates(H)))                \n",
    "                G0 = nx.subgraph(G0, H.nodes())\n",
    "                G1 = nx.subgraph(G1, H.nodes())\n",
    "\n",
    "            modularities0 = []\n",
    "            modularities1 = []\n",
    "\n",
    "            for seed in range(10):\n",
    "                communities0 = nx.community.louvain_communities(G0, seed=seed)\n",
    "                modularities0.append(nx.community.modularity(G0, communities0))\n",
    "\n",
    "                communities1 = nx.community.louvain_communities(G1, seed=seed)\n",
    "                modularities1.append(nx.community.modularity(G1, communities1))\n",
    "\n",
    "\n",
    "            t, p = stats.ttest_ind(modularities0, modularities1, equal_var=False, alternative='less')\n",
    "\n",
    "            print(f'{temp[\"name\"]}, {temp[\"temperature\"]}, T-test: {t}, p-value: {p}')\n",
    "\n",
    "def lcc(G):\n",
    "    Gcc = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "    return G.subgraph(Gcc[0])\n",
    "\n",
    "def small_worldness(filenames, name, dataloader_fn, subgraph=False):\n",
    "\n",
    "    networks = dataloader_fn()\n",
    "\n",
    "    G_initial = networks[list(networks.keys())[0]]\n",
    "\n",
    "    # LCC subgraph\n",
    "    G_initial = lcc(G_initial)\n",
    "\n",
    "    average_shortest_path_length_initial = nx.average_shortest_path_length(G_initial)\n",
    "    clustering_coefficient_initial = nx.average_clustering(G_initial)\n",
    "\n",
    "    for filename in filenames:\n",
    "\n",
    "            with open(filename) as f:\n",
    "                lines = f.read().splitlines()\n",
    "\n",
    "            for line in lines:\n",
    "                temp = json.loads(line)\n",
    "\n",
    "                G0 = nx.from_dict_of_dicts(temp['graphs'][0])\n",
    "                G1 = nx.from_dict_of_dicts(temp['graphs'][-1])\n",
    "\n",
    "                G1 = lcc(G1)\n",
    "                # if subgraph:\n",
    "                #     H = nx.difference(G1, G0)\n",
    "                #     H.remove_nodes_from(list(nx.isolates(H)))                \n",
    "                #     G0 = nx.subgraph(G0, H.nodes())\n",
    "                #     G1 = nx.subgraph(G1, H.nodes())\n",
    "\n",
    "                average_shortest_path_length = nx.average_shortest_path_length(G1)\n",
    "                clustering_coefficient = nx.average_clustering(G1)\n",
    "\n",
    "                average_shortest_path_length_initial_change = (average_shortest_path_length - average_shortest_path_length_initial) / average_shortest_path_length_initial * 100\n",
    "                clustering_coefficient_initial_change = (clustering_coefficient - clustering_coefficient_initial) / clustering_coefficient_initial * 100\n",
    "\n",
    "                print(f'{temp[\"name\"]}, {temp[\"temperature\"]}, Average Shortest Path Length Change: {average_shortest_path_length_initial_change}, Clustering Coefficient Change: {clustering_coefficient_initial_change}')\n",
    "\n",
    "def measure_relative_increase(filenames):\n",
    "\n",
    "    for filename in filenames:\n",
    "\n",
    "        with open(filename) as f:\n",
    "            lines = f.read().splitlines()\n",
    "\n",
    "        data = []\n",
    "\n",
    "        for line in lines:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "        for d in data:\n",
    "            total, count = 0, 0        \n",
    "            for results in d[\"results\"]:\n",
    "                for result in results:\n",
    "                    count += int(result['dropped'])\n",
    "                    total += 1          \n",
    "\n",
    "            accuracy = count / total * 100\n",
    "            random_guess = 100 / d[\"num_choices\"]\n",
    "            relative_increase = (accuracy - random_guess) / random_guess * 100\n",
    "\n",
    "            print(f'{d[\"name\"]}, {d[\"temperature\"]}, {d[\"simulation\"]}, Relative Increase in Accuracy % = {relative_increase}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulations\n",
    "models = ['gpt-3.5-turbo', 'meta/meta-llama-3-70b-instruct', 'gpt-4o-mini', 'claude-3-5-sonnet-20240620']\n",
    "datasets = ['Caltech36', 'Swarthmore42', 'UChicago30', 'ego-facebook', 'ego-twitter', 'ego-gplus']\n",
    "\n",
    "num_egonets = -1\n",
    "egonets_radius = -1\n",
    "sample_egonets = False\n",
    "\n",
    "for model in models:\n",
    "    print(model)\n",
    "    for name in datasets:\n",
    "        for i, temperature in enumerate([0.5, 1.0, 1.5], 1):\n",
    "            outfile = f'outputs/combined_model_facebook100_{name.lower()}_whole_{i}+{model.replace(\"/\", \"-\")}.jsonl'\n",
    "            if name.startswith('ego'):\n",
    "                dataloader_fn = lambda: dataloader.load_snap_ego_nets(input_dir='datasets', name=name.lstrip('ego-'))\n",
    "            else:\n",
    "                dataloader_fn = lambda: dataloader.load_facebook100(input_dir='datasets/facebook100', name=name, num_egonets=num_egonets, egonets_radius=egonets_radius, sample_egonets=sample_egonets)\n",
    "            \n",
    "            dataloader_fn()\n",
    "            \n",
    "            # run_network_formation_experiment(name=name, num_nodes_samples=3000, num_simulations=1, outfile=outfile, temperatures=[temperature], method='llm', num_choices=1, num_samples=15, model=model, dataloader_fn=dataloader_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and format regression tables\n",
    "for model in models:\n",
    "    for name in datasets:\n",
    "        for i in range(1, 4):\n",
    "            generate_regression_table(f'outputs/combined_model_facebook100_{name.lower()}_whole_{i}+{model.replace(\"/\", \"-\")}.jsonl', f'tables/combined_model_facebook100_{name.lower()}_whole_{i}+{model.replace(\"/\", \"-\")}.xlsx')\n",
    "\n",
    "# Export regression tables to LaTeX\n",
    "for model in models:\n",
    "    for name in datasets:\n",
    "        for full in [True, False]:\n",
    "            pretty_print_regression_table([f'tables/combined_model_facebook100_{name.lower()}_whole_{i}+{model.replace(\"/\", \"-\")}.xlsx' for i in range(1, 4)], f'tables/combined_model_facebook100_{name.lower()}{\"_full\" if full else \"\"}+{model.replace(\"/\", \"-\")}.tex', full=full)\n",
    "\n",
    "    plot_coefficients([f'tables/combined_model_facebook100_{name.lower()}_whole_{i}+{model.replace(\"/\", \"-\")}.xlsx' for i in range(1, 4) for name in datasets], f'figures/combined_model_facebook100_coefficients+{model.replace(\"/\", \"-\")}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Community structure\n",
    "for model in models:\n",
    "    for name in datasets:\n",
    "        modularity_change([f'outputs/combined_model_facebook100_{name.lower()}_whole_{i}+{model.replace(\"/\", \"-\")}.jsonl' for i in range(1, 4)], subgraph=False)\n",
    "        if name == 'UChicago30':\n",
    "            modularity_change([f'outputs/combined_model_facebook100_{name.lower()}_whole_{i}+{model.replace(\"/\", \"-\")}.jsonl' for i in range(1, 4)], subgraph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_egonets = -1\n",
    "egonets_radius = -1\n",
    "sample_egonets = False\n",
    "\n",
    "for model in models:\n",
    "    for name in datasets:\n",
    "        dataloader_fn = lambda: dataloader.load_facebook100(input_dir='datasets/facebook100', name=name, num_egonets=num_egonets, egonets_radius=egonets_radius, sample_egonets=sample_egonets)\n",
    "        small_worldness([f'outputs/combined_model_facebook100_{name.lower()}_whole_{i}+{model.replace(\"/\", \"-\")}.jsonl' for i in range(1, 4)], name, subgraph=False, dataloader_fn=dataloader_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative increase in accuracy compared to random baseline\n",
    "for model in models:\n",
    "    for name in ['Caltech36', 'Swarthmore42', 'UChicago30']:\n",
    "        measure_relative_increase([f'outputs/combined_model_facebook100_{name.lower()}_whole_{i}_small+{model.replace(\"/\", \"-\")}.jsonl' for i in range(1, 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM reasoning\n",
    "\n",
    "# categories = [{'category' : 'Number of friends', 'description' : 'Large number of friends'},\n",
    "#               {'category' : 'Mutual friends', 'description' : 'Large number of mutual friends'},\n",
    "#               {'category' : 'Similar attributes', 'description' : 'Similar attributes such as major, faculty status, etc.'}]\n",
    "\n",
    "# for model in models:\n",
    "#     for name in ['Caltech36', 'Swarthmore42', 'UChicago30']:\n",
    "#         summarize_reasons([f'outputs/combined_model_facebook100_{name.lower()}_whole_{i}.jsonl' for i in range(1, 4)], 'UChicago30', n_samples=20, n_categories=3, n_resamples=2, categories=categories)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
