{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import scipy.stats as stats\n",
    "import netgraph\n",
    "\n",
    "import seaborn as sns\n",
    "import json\n",
    "import openai\n",
    "import random\n",
    "import os\n",
    "import collections\n",
    "import copy\n",
    "from utils import get_response, summarize_reasons\n",
    "\n",
    "\n",
    "MEDIUM_SIZE = 28\n",
    "SMALL_SIZE = 0.85 * MEDIUM_SIZE\n",
    "BIGGER_SIZE = 1.5 * MEDIUM_SIZE\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "def generate_individuals(n):\n",
    "    profiles = []\n",
    "\n",
    "    hobbies = ['reading', 'writing', 'cooking']\n",
    "    colors = ['red', 'orange', 'yellow', 'green']\n",
    "    \n",
    "    locations = ['New York City', 'Boston', 'Washington DC']\n",
    "\n",
    "    for i in range(n):\n",
    "        profile = {\n",
    "            'name' : i,\n",
    "            'hobby' : random.choice(hobbies),\n",
    "            'favorite color' : random.choice(colors),\n",
    "            'location' : random.choice(locations)\n",
    "        }\n",
    "\n",
    "        profiles.append(profile)\n",
    "\n",
    "    with open('outputs/profiles.jsonl', 'w+') as f:\n",
    "        [f.write(json.dumps(profile) + '\\n') for profile in profiles]\n",
    "\n",
    "\n",
    "def network_growth(n0, temperature, model, environment, role, method='llm', cot=False, profiles_filename='outputs/profiles.jsonl', mutual_acceptance=False):\n",
    "    with open(profiles_filename) as f:\n",
    "        profiles = f.read().splitlines()\n",
    "        profiles = [json.loads(profile) for profile in profiles]\n",
    "\n",
    "    profiles = profiles[:n0]\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(n0))\n",
    "\n",
    "    Gs = []\n",
    "    results = []\n",
    "    total_requests = 0\n",
    "    num_accepted_requests = 0\n",
    "\n",
    "    for t in range(n0):\n",
    "\n",
    "        if method == 'llm':\n",
    "            result = select_neighbor(G, t, profiles, temperature, model=model, environment=environment, role=role, cot=cot, mutual_acceptance=mutual_acceptance)\n",
    "\n",
    "            if result:\n",
    "                for r in result:\n",
    "                    v = r['name']\n",
    "                    if r.get('accepted', False):\n",
    "                        G.add_edge(t, v, similarity=r['similarity'])\n",
    "                        num_accepted_requests += 1\n",
    "                    total_requests += 1\n",
    "                results.append(result)\n",
    "        elif method in ['random', 'homophilous', 'heterophilous']:\n",
    "            if method == 'random':\n",
    "                new_nodes = random.sample(list(set(G.nodes()) - set([t])), 4)\n",
    "            elif method == 'homophilous':\n",
    "                new_nodes = list(sorted([v for v in G.nodes() if v != t], key=lambda v: len(set(profile_set(profiles[t])) & profile_set(profiles[v])), reverse=True))[:4]\n",
    "            elif method == 'heterophilous':\n",
    "                new_nodes = list(sorted([v for v in G.nodes() if v != t], key=lambda v: len(set(profile_set(profiles[t])) & profile_set(profiles[v]))))[:4]\n",
    "            \n",
    "            for v in new_nodes:\n",
    "                intersection = list(set(profile_set(profiles[t])) & profile_set(profiles[v]))\n",
    "                union = list(set(profile_set(profiles[t])) | profile_set(profiles[v]))\n",
    "                similarity = len(intersection)\n",
    "                G.add_edge(t, v, intersection=intersection, union=union, similarity=similarity)\n",
    "                num_accepted_requests += 1\n",
    "                total_requests += 1\n",
    "            \n",
    "            results.append([{'name' : v, 'intersection' : intersection, 'union' : union, 'similarity' : similarity} for v in new_nodes])\n",
    "       \n",
    "        Gs.append(G.copy())\n",
    "\n",
    "    return Gs, results, num_accepted_requests / total_requests * 100\n",
    "\n",
    "def profile_set(p):\n",
    "    temp = []\n",
    "    for k, v in p.items():\n",
    "        if k == 'name':\n",
    "            continue\n",
    "        else:\n",
    "            temp.append(v)\n",
    "\n",
    "    return set(temp)\n",
    "\n",
    "def select_neighbor(G, t, profiles, temperature, model, environment, role, cot, mutual_acceptance=False):\n",
    "    candidate_profiles = []\n",
    "    for v in G.nodes():\n",
    "        if v != t:          \n",
    "            candidate_profiles.append(profiles[v])\n",
    "    \n",
    "    if cot: \n",
    "        output_format = f\"\"\"\n",
    "    {{\n",
    "        \"reason\" : reason for selecting the person,\n",
    "        \"name\" : name of the person you selected\n",
    "    }}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        output_format = f\"\"\"\n",
    "    {{\n",
    "        \"name\" : name of the person you selected,\n",
    "        \"reason\" : reason for selecting the person\n",
    "    }}\n",
    "        \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    # Task\n",
    "    {f'You are in a {environment}.' if environment else ''}Your task is to select a person to be {role} with.\n",
    "   \n",
    "    # Profile\n",
    "    Your profile is given below after chevrons:\n",
    "    <PROFILE>\n",
    "    {json.dumps(profiles[t])}\n",
    "    </PROFILE>\n",
    "\n",
    "    # Candidate Profiles\n",
    "    The cadidate profiles to be friends with are given below after chevrons:\n",
    "\n",
    "    <PROFILES>\n",
    "    {json.dumps(candidate_profiles)}\n",
    "    </PROFILES>\n",
    "\n",
    "    # Output\n",
    "    The output should be given a list of JSON objects with the following structure\n",
    "\n",
    "    [\n",
    "        {output_format}, ...\n",
    "    ]\n",
    "\n",
    "    # Notes\n",
    "    * The output must be a list of JSON objects ranked in the order of preference.\n",
    "    * You can make at most 4 selections.\n",
    "    * Your output mush be only the JSON objects and nothing else.\n",
    "\n",
    "    ```json\n",
    "    \"\"\"   \n",
    "\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            ans = get_response(prompt, model, temperature)\n",
    "            # import pdb; pdb.set_trace()\n",
    "            try:\n",
    "                results = json.loads(ans.split('```')[0])\n",
    "            except:\n",
    "                results = json.loads(ans.split('```json')[1].split('```')[0])\n",
    "\n",
    "            filtered_results = []\n",
    "            for result in results:\n",
    "                if result['name'] in G.nodes():\n",
    "                    v = result['name']\n",
    "                    result['intersection'] = list(profile_set(profiles[t]) & profile_set(profiles[v]))\n",
    "                    result['union'] = list(profile_set(profiles[t]) | profile_set(profiles[v]))\n",
    "                    result['similarity'] = len(result['intersection'])\n",
    "                    filtered_results.append(result)\n",
    "\n",
    "            break            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    for r in filtered_results:\n",
    "        if mutual_acceptance:\n",
    "            prompt = f\"\"\"\n",
    "            # Task\n",
    "            {f'You are in a {environment}.' if environment else ''}. You receive a request to be {role} with a person.\n",
    "        \n",
    "            # Profile\n",
    "            Your profile is given below after chevrons:\n",
    "            <PROFILE>\n",
    "            {json.dumps(profiles[t])}\n",
    "            </PROFILE>\n",
    "\n",
    "            # Candidate Profile\n",
    "            The candidate profile to be friends with is given below after chevrons:\n",
    "            <PROFILE>\n",
    "            {json.dumps(profiles[r[\"name\"]])}\n",
    "            </PROFILE>\n",
    "\n",
    "            # Output\n",
    "            The output should be a JSON object with the following structure\n",
    "\n",
    "            {{\n",
    "                \"accept\" : true/false,\n",
    "                \"reason\" : reason for accepting or rejecting the request\n",
    "            }}\n",
    "\n",
    "            # Notes\n",
    "            * The output must be a JSON object.\n",
    "            * You can only accept or reject the request.\n",
    "            * Your output mush be only the JSON object and nothing else.\n",
    "\n",
    "            ```json\n",
    "            \"\"\"\n",
    "\n",
    "            for i in range(10):\n",
    "                try:\n",
    "                    ans = get_response(prompt, model, temperature)\n",
    "                    try:\n",
    "                        results = json.loads(ans.split('```')[0])\n",
    "                    except:\n",
    "                        results = json.loads(ans.split('```json')[1].split('```')[0])\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "                if results['accept']:\n",
    "                    # set result accepted to true\n",
    "                    r['accepted'] = True\n",
    "                    r['acceptance_reason'] = results['reason']\n",
    "                    break\n",
    "        else:\n",
    "            r['accepted'] = True\n",
    "            r['acceptance_reason'] = 'Mutual acceptance is not enabled'\n",
    "\n",
    "    print(f'Node: {t}, Results: {filtered_results}')\n",
    "\n",
    "    return filtered_results\n",
    "\n",
    "\n",
    "def run_network_formation_experiment(n_min, n_max, n_step, num_simulations, outfile, temperatures, method, model, environment, role, cot=False, profiles_filename='outputs/profiles.jsonl', mutual_acceptance=False):\n",
    "    saved_scenarios = set()\n",
    "\n",
    "    if os.path.exists(outfile):\n",
    "        with open(outfile) as f:\n",
    "            lines = f.read().splitlines()\n",
    "\n",
    "            for line in lines:\n",
    "                scenario = json.loads(line)\n",
    "                saved_scenarios.add((scenario['n'], scenario['simulation'], scenario['temperature']))\n",
    "\n",
    "    f = open(outfile, 'a+')\n",
    "\n",
    "\n",
    "    for n in range(n_min, n_max + 1, n_step):\n",
    "        for i in range(num_simulations):\n",
    "            for temperature in temperatures:\n",
    "                if (n, i, temperature) in saved_scenarios:\n",
    "                    print(f'Skipping simulation for n={n}, i={i}, temperature={temperature}')\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f'Running simulation for n={n}, i={i}, temperature={temperature}')\n",
    "\n",
    "                    Gs, reasons, mutual_acceptance_probability = network_growth(n, temperature=temperature, method=method, model=model, environment=environment, role=role, cot=cot, profiles_filename=profiles_filename, mutual_acceptance=mutual_acceptance)\n",
    "\n",
    "                    temp = {\n",
    "                        'n' : n,\n",
    "                        'temperature' : temperature,\n",
    "                        'simulation' : i,\n",
    "                        'graphs' : [nx.to_dict_of_dicts(G) for G in Gs],\n",
    "                        'reasons' : reasons,\n",
    "                        'mutual_acceptance_probability' : mutual_acceptance_probability\n",
    "                    }    \n",
    "\n",
    "                    f.write(json.dumps(temp) + '\\n')            \n",
    "\n",
    "                if method != 'llm':\n",
    "                    break\n",
    "\n",
    "    f.close()\n",
    "\n",
    "def draw_graph(G, ax, communities=None, palette=None, use_netgraph=True, ego_network=False):\n",
    "\n",
    "    if not use_netgraph:\n",
    "        pos = nx.spring_layout(G, weight='similarity')\n",
    "    \n",
    "        if communities:\n",
    "            for i, community in enumerate(communities):\n",
    "                nx.draw_networkx_nodes(G, pos, nodelist=list(community), node_size=20, node_color=palette[i], ax=ax)\n",
    "        else:\n",
    "            nx.draw_networkx_nodes(G, pos, nodelist=list(G.nodes()), node_size=20, node_color='#d35400', ax=ax)\n",
    "\n",
    "        edge_widths = [1 + G.edges[u, v]['similarity'] for (u, v) in G.edges()]\n",
    "\n",
    "        nx.draw_networkx_edges(G, pos, edgelist=list(G.edges()), width=edge_widths, edge_color='#34495e', alpha=0.5, ax=ax)\n",
    "    elif use_netgraph and not ego_network:\n",
    "        node2community = {node: i for i, community in enumerate(communities) for node in community}\n",
    "        node_color = {node : palette[node2community[node]] for node in G.nodes()}\n",
    "        edge_widths = {(u, v) : (1 + G.edges[u, v]['similarity']) * 0.5 for (u, v) in G.edges()}\n",
    "\n",
    "        # netgraph.Graph(G, node_layout='community', node_color=node_color, node_layout_kwargs=dict(node_to_community=node2community), node_size=2.5, edge_width=edge_widths, edge_color='#34495e', edge_layout='bundled', edge_layout_kwargs=dict(k=2000), ax=ax)\n",
    "        netgraph.Graph(G, node_layout='community', node_color=node_color, node_layout_kwargs=dict(node_to_community=node2community), node_size=2.5, edge_width=edge_widths, edge_color='#34495e', ax=ax)\n",
    "\n",
    "    elif use_netgraph and ego_network:\n",
    "        ego_net = nx.ego_graph(G, list(G.nodes())[0], radius=1)\n",
    "\n",
    "        node2community = {node: i for i, community in enumerate(communities) for node in community}\n",
    "\n",
    "        node_color = {node : palette[node2community[node]] for node in ego_net.nodes()}\n",
    "        node_size = {}\n",
    "\n",
    "        for node in ego_net.nodes():\n",
    "            if node == list(G.nodes())[0]:\n",
    "                node_size[node] = 5\n",
    "            else:\n",
    "                node_size[node] = 2.5\n",
    "\n",
    "        edge_widths = {(u, v) : (1 + ego_net.edges[u, v]['similarity']) * 0.5 for (u, v) in ego_net.edges()}\n",
    "        netgraph.Graph(ego_net, node_layout='community', node_color=node_color, node_layout_kwargs=dict(node_to_community=node2community), node_size=node_size, edge_width=edge_widths, edge_color='#34495e', ax=ax)\n",
    "\n",
    "    ax.set_axis_off()\n",
    "\n",
    "\n",
    "def analyze_experiments(filename):\n",
    "\n",
    "    palette = ['#d35400', '#34495e', '#2980b9', '#e67e22', '#f1c40f', '#7f8c8d', '#27ae60', '#16a085', '#bdc3c7', '#1abc9c', '#2ecc71', '#3498db', '#9b59b6', '#8e44ad', '#ecf0f1']\n",
    "\n",
    "    with open(filename) as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    data = []\n",
    "\n",
    "\n",
    "    for line in lines:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "    edge_similarity_distributions = { 'homophilous' : collections.defaultdict(list), 'random' : collections.defaultdict(list), 'heterophilous' : collections.defaultdict(list), 'llm' : collections.defaultdict(list) }\n",
    "    wasserstein_distance = { 'homophilous' : collections.defaultdict(list), 'random' : collections.defaultdict(list), 'heterophilous' : collections.defaultdict(list) }\n",
    "    louvain_communities = { 'homophilous' : collections.defaultdict(list), 'random' : collections.defaultdict(list), 'heterophilous' : collections.defaultdict(list), 'llm' : collections.defaultdict(list) }\n",
    "    louvain_modularity = { 'homophilous' : collections.defaultdict(list), 'random' : collections.defaultdict(list), 'heterophilous' : collections.defaultdict(list), 'llm' : collections.defaultdict(list) }\n",
    "    location_assortativities = { 'homophilous' : collections.defaultdict(list), 'random' : collections.defaultdict(list), 'heterophilous' : collections.defaultdict(list), 'llm' : collections.defaultdict(list)}\n",
    "    favorite_color_assortativities = { 'homophilous' : collections.defaultdict(list), 'random' : collections.defaultdict(list), 'heterophilous' : collections.defaultdict(list), 'llm' : collections.defaultdict(list)}\n",
    "    hobby_assortativities = { 'homophilous' : collections.defaultdict(list), 'random' : collections.defaultdict(list), 'heterophilous' : collections.defaultdict(list), 'llm' : collections.defaultdict(list)}\n",
    "\n",
    "    final_graphs = collections.defaultdict(list)\n",
    "\n",
    "    with open('outputs/profiles.jsonl') as f:\n",
    "        profiles = f.read().splitlines()\n",
    "        profiles = [json.loads(profile) for profile in profiles]\n",
    "\n",
    "    profiles_dict = {str(profile['name']) : profile for profile in profiles}\n",
    "\n",
    "    for d in data:\n",
    "        Gs = []\n",
    "        for graph in d['graphs']:\n",
    "            G = nx.from_dict_of_dicts(graph)\n",
    "            \n",
    "            nx.set_node_attributes(G, profiles_dict)\n",
    "\n",
    "            G.remove_edges_from(nx.selfloop_edges(G))\n",
    "            # G.remove_nodes_from(list(nx.isolates(G)))\n",
    "\n",
    "            Gs.append(G)\n",
    "\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "        louvain_communities['llm'][d['n'], d['temperature']].append(nx.algorithms.community.louvain_communities(Gs[-1], weight='similarity'))\n",
    "        louvain_modularity['llm'][d['n'], d['temperature']].append(nx.algorithms.community.modularity(Gs[-1], louvain_communities['llm'][d['n'], d['temperature']][-1], weight='similarity'))\n",
    "\n",
    "\n",
    "        final_graphs[d['n'], d['temperature']].append((Gs[-1], louvain_communities['llm'][d['n'], d['temperature']][-1]))\n",
    "\n",
    "\n",
    "        G_homophilous = network_growth(d['n'], d['temperature'], method='homophilous')[0][-1]\n",
    "        G_heterophilous = network_growth(d['n'], d['temperature'], method='heterophilous')[0][-1]\n",
    "        G_random = network_growth(d['n'], d['temperature'], method='random')[0][-1]\n",
    "\n",
    "        nx.set_node_attributes(G_homophilous, {int(profile['name']) : profile for profile in profiles})\n",
    "        nx.set_node_attributes(G_heterophilous, {int(profile['name']) : profile for profile in profiles})\n",
    "        nx.set_node_attributes(G_random, {int(profile['name']) : profile for profile in profiles})\n",
    "\n",
    "        ax[0].set_title(f'Temperature = {d[\"temperature\"]}')\n",
    "        \n",
    "        draw_graph(Gs[-1], ax=ax[0], communities=louvain_communities['llm'][d['n'], d['temperature']][-1], palette=palette)\n",
    "\n",
    "        edge_similarity_distribution = [G.edges[u, v]['similarity'] for (u, v) in Gs[-1].edges()]\n",
    "        edge_similarity_distribution_homophilous = [G_homophilous.edges[u, v]['similarity'] for (u, v) in G_homophilous.edges()]\n",
    "        edge_similarity_distribution_heterophilous = [G_heterophilous.edges[u, v]['similarity'] for (u, v) in G_heterophilous.edges()]\n",
    "        edge_similarity_distribution_random = [G_random.edges[u, v]['similarity'] for (u, v) in G_random.edges()]\n",
    "\n",
    "        wasserstein_distance['homophilous'][d['n'], d['temperature']].append(stats.wasserstein_distance(edge_similarity_distribution, edge_similarity_distribution_homophilous))\n",
    "        wasserstein_distance['heterophilous'][d['n'], d['temperature']].append(stats.wasserstein_distance(edge_similarity_distribution, edge_similarity_distribution_heterophilous))\n",
    "        wasserstein_distance['random'][d['n'], d['temperature']].append(stats.wasserstein_distance(edge_similarity_distribution, edge_similarity_distribution_random))\n",
    "\n",
    "        print(f'Temperature: {d[\"temperature\"]} T-test Test Homophilous: {stats.ttest_ind(edge_similarity_distribution, edge_similarity_distribution_homophilous, equal_var=False, alternative=\"less\")}')\n",
    "        print(f'Temperature: {d[\"temperature\"]} T-test Test Random: {stats.ttest_ind(edge_similarity_distribution, edge_similarity_distribution_random, equal_var=False, alternative=\"two-sided\")}')\n",
    "    \n",
    "\n",
    "        edge_similarity_distributions['homophilous'][d['n'], d['temperature']].extend(edge_similarity_distribution_homophilous)\n",
    "        edge_similarity_distributions['heterophilous'][d['n'], d['temperature']].extend(edge_similarity_distribution_heterophilous)\n",
    "        edge_similarity_distributions['random'][d['n'], d['temperature']].extend(edge_similarity_distribution_random)\n",
    "        edge_similarity_distributions['llm'][d['n'], d['temperature']].extend(edge_similarity_distribution)\n",
    "\n",
    "        louvain_communities['homophilous'][d['n'], d['temperature']].append(nx.algorithms.community.louvain_communities(G_homophilous, weight='similarity'))\n",
    "        louvain_communities['random'][d['n'], d['temperature']].append(nx.algorithms.community.louvain_communities(G_random, weight='similarity'))\n",
    "\n",
    "        louvain_modularity['homophilous'][d['n'], d['temperature']].append(nx.algorithms.community.modularity(G_homophilous, louvain_communities['homophilous'][d['n'], d['temperature']][-1], weight='similarity'))\n",
    "        louvain_modularity['random'][d['n'], d['temperature']].append(nx.algorithms.community.modularity(G_random, louvain_communities['random'][d['n'], d['temperature']][-1], weight='similarity'))\n",
    "\n",
    "        location_assortativities['homophilous'][d['n'], d['temperature']].append(nx.attribute_assortativity_coefficient(G_homophilous, 'location'))\n",
    "        location_assortativities['heterophilous'][d['n'], d['temperature']].append(nx.attribute_assortativity_coefficient(G_heterophilous, 'location'))\n",
    "        location_assortativities['random'][d['n'], d['temperature']].append(nx.attribute_assortativity_coefficient(G_random, 'location'))\n",
    "        location_assortativities['llm'][d['n'], d['temperature']].append(nx.attribute_assortativity_coefficient(Gs[-1], 'location'))\n",
    "\n",
    "        favorite_color_assortativities['homophilous'][d['n'], d['temperature']].append(nx.attribute_assortativity_coefficient(G_homophilous, 'favorite color'))\n",
    "        favorite_color_assortativities['heterophilous'][d['n'], d['temperature']].append(nx.attribute_assortativity_coefficient(G_heterophilous, 'favorite color'))\n",
    "        favorite_color_assortativities['random'][d['n'], d['temperature']].append(nx.attribute_assortativity_coefficient(G_random, 'favorite color'))\n",
    "        favorite_color_assortativities['llm'][d['n'], d['temperature']].append(nx.attribute_assortativity_coefficient(Gs[-1], 'favorite color'))\n",
    "\n",
    "        hobby_assortativities['homophilous'][d['n'], d['temperature']].append(nx.attribute_assortativity_coefficient(G_homophilous, 'hobby'))\n",
    "        hobby_assortativities['heterophilous'][d['n'], d['temperature']].append(nx.attribute_assortativity_coefficient(G_heterophilous, 'hobby'))\n",
    "        hobby_assortativities['random'][d['n'], d['temperature']].append(nx.attribute_assortativity_coefficient(G_random, 'hobby'))\n",
    "        hobby_assortativities['llm'][d['n'], d['temperature']].append(nx.attribute_assortativity_coefficient(Gs[-1], 'hobby'))\n",
    "\n",
    "\n",
    "        sns.histplot(edge_similarity_distribution, ax=ax[1], label='LLM', color='#d35400', binwidth=0.45, discrete=True, stat='density')\n",
    "        ax[1].xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
    "        \n",
    "        ax[1].set_xlabel('Number of Common Attributes')\n",
    "        ax[1].set_ylabel('Probability of Edge Creation')\n",
    "        ax[1].set_ylim(0, 0.75)\n",
    "        ax[1].legend()\n",
    "\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f'figures/principle_3/principle_3_profiles_{d[\"n\"]}_{d[\"simulation\"]}_{d[\"temperature\"]}.png', dpi=300)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "\n",
    "    ax[0].set_xlabel('Temperature')\n",
    "    ax[0].set_ylabel('Wasserstein Distance')\n",
    "\n",
    "\n",
    "    wasserstein_distance_means = { 'homophilous' : [], 'heterophilous' : [], 'random' : [] }\n",
    "    wasserstein_distance_stds = { 'homophilous' : [],  'heterophilous' : [], 'random' : [] }\n",
    "\n",
    "    for method, color in zip(['homophilous', 'heterophilous', 'random'], palette[:3]):\n",
    "        for i, k in enumerate(sorted(wasserstein_distance[method].keys())):\n",
    "            v = np.array(wasserstein_distance[method][k])\n",
    "            mean = v.mean()\n",
    "            std = v.std()\n",
    "\n",
    "            wasserstein_distance_means[method].append(mean)\n",
    "            wasserstein_distance_stds[method].append(std)\n",
    "\n",
    "    for method, color in zip(['homophilous', 'random'], palette[:3]):\n",
    "        \n",
    "        ax[0].plot(np.arange(len(wasserstein_distance[method])), wasserstein_distance_means[method], label=method.capitalize(), marker='o', color=color)\n",
    "        ax[0].fill_between(np.arange(len(wasserstein_distance[method])), np.array(wasserstein_distance_means[method]) - 1.96 * np.array(wasserstein_distance_stds[method]) / np.sqrt(len(wasserstein_distance[method])), np.array(wasserstein_distance_means[method]) + 1.96 * np.array(wasserstein_distance_stds[method]) / np.sqrt(len(wasserstein_distance[method])), alpha=0.2, color=color)\n",
    "\n",
    "    ax[0].set_xticks(np.arange(len(wasserstein_distance['homophilous'])))\n",
    "    ax[0].set_xticklabels([f'{k[1]}' for k in sorted(wasserstein_distance['homophilous'].keys())])\n",
    "    ax[0].legend()\n",
    "    ax[0].set_ylabel(f'Wasserstein Distance')\n",
    "\n",
    "\n",
    "    objs = []\n",
    "\n",
    "    for i, k in enumerate(sorted(edge_similarity_distributions['llm'].keys())):\n",
    "        objs.append(pd.DataFrame.from_dict({'Number of Common Attributes' : edge_similarity_distributions['llm'][k], 'Method' : f'{k[1]}'}))\n",
    "\n",
    "        if i == len(edge_similarity_distributions['llm'].keys()) - 1:\n",
    "            for method in ['homophilous', 'random']:\n",
    "                objs.append(pd.DataFrame.from_dict({'Number of Common Attributes' : edge_similarity_distributions[method][k], 'Method' : method.capitalize()}))\n",
    "\n",
    "\n",
    "    df = pd.concat(axis=0, ignore_index=True, objs=objs)\n",
    "\n",
    "    sns.histplot(\n",
    "        data=df, x='Number of Common Attributes', hue='Method', multiple='dodge', palette=palette,\n",
    "        bins=range(4), ax=ax[1], discrete=True, shrink=0.8, stat='probability', common_norm=False\n",
    "    )\n",
    "\n",
    "    sns.move_legend(ax[1], bbox_to_anchor=(1, 0.5), loc='center left', frameon=False)\n",
    "\n",
    "    ax[1].set_ylabel('Probability of Edge Creation')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    fig.savefig('figures/principle_3/principle_3_profiles_overall.png', dpi=300)\n",
    "\n",
    "    fig, ax = plt.subplots(1, len(final_graphs), figsize=(5 * (len(final_graphs)), 5))\n",
    "\n",
    "    for i, (k, v) in enumerate(sorted(final_graphs.items())):\n",
    "        G = v[0][0]\n",
    "        communities = v[0][1]        \n",
    "        ax[i].set_title(f\"Temperature = {k[-1]}\", fontsize=MEDIUM_SIZE)\n",
    "        draw_graph(G, ax=ax[i], communities=communities, palette=palette)\n",
    "\n",
    "\n",
    "    # sns.histplot(\n",
    "    #     data=df, x='Number of Common Attributes', hue='Method', multiple='dodge',\n",
    "    #     bins=range(4), ax=ax[-1], discrete=True, shrink=0.8, stat='probability', common_norm=False, palette=palette\n",
    "    # )\n",
    "\n",
    "    # sns.barplot(\n",
    "    #     data=df, y='Number of Common Attributes', x='Method', ax=ax[-1], palette=palette\n",
    "    # )\n",
    "\n",
    "    # plt.xticks(fontsize=SMALL_SIZE, rotation=90)\n",
    "\n",
    "\n",
    "    # sns.move_legend(ax[-1], bbox_to_anchor=(1, 0.5), loc='center left', frameon=False)\n",
    "\n",
    "    # ax[-1].set_ylabel('# Common Attributes')\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig.savefig('figures/principle_3/principle_3_profiles_final_graphs.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    ax[0].set_ylabel('Louvain Modularity')\n",
    "    ax[1].set_ylabel('Num. of Communities')\n",
    "    ax[2].set_ylabel('Community Size')\n",
    "\n",
    "    ax[0].spines[['right', 'top']].set_visible(False)\n",
    "    ax[1].spines[['right', 'top']].set_visible(False)\n",
    "    ax[2].spines[['right', 'top']].set_visible(False)\n",
    "\n",
    "    j = 0\n",
    "\n",
    "    for method in ['llm']:\n",
    "        for i, k in enumerate(sorted(louvain_modularity[method].keys())):\n",
    "            if method != 'llm' and i != 0:\n",
    "                continue\n",
    "\n",
    "            modularities = np.array(louvain_modularity[method][k])\n",
    "            number_of_communities = np.array([len(c) for c in louvain_communities[method][k]])\n",
    "            community_sizes = np.array([len(v) for c in louvain_communities[method][k] for v in c])\n",
    "\n",
    "\n",
    "            if method == 'llm':\n",
    "                label = f'{k[-1]}'\n",
    "            else:\n",
    "                label = method.capitalize()\n",
    "\n",
    "            mean_modularity = modularities.mean()\n",
    "            ci_modularity = 1.96 * modularities.std() / np.sqrt(len(modularities))\n",
    "\n",
    "            mean_number_of_communities = number_of_communities.mean()\n",
    "            ci_number_of_communities = 1.96 * number_of_communities.std() / np.sqrt(len(number_of_communities))\n",
    "\n",
    "            mean_community_sizes = community_sizes.mean()\n",
    "            ci_community_sizes = 1.96 * community_sizes.std() / np.sqrt(len(community_sizes))\n",
    "\n",
    "            # ax[0].bar(i, mean_modularity, yerr=ci_modularity, label=label, color=palette[j])\n",
    "            # ax[1].bar(i, mean_number_of_communities, yerr=ci_number_of_communities, label=label, color=palette[j])\n",
    "            # ax[2].bar(i, mean_community_sizes, yerr=ci_community_sizes, label=label, color=palette[j])\n",
    "\n",
    "\n",
    "            ax[0].errorbar(k[-1], mean_modularity, yerr=ci_modularity, fmt='o', label=label, color=palette[j], capsize=5)\n",
    "            ax[1].errorbar(k[-1], mean_number_of_communities, yerr=ci_number_of_communities, fmt='o', label=label, color=palette[j], capsize=5)\n",
    "            ax[2].errorbar(k[-1], mean_community_sizes, yerr=ci_community_sizes, fmt='o', label=label, color=palette[j], capsize=5)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "            # sns.distplot(modularities, hist=False, ax=ax[0], label=label, color=palette[j])\n",
    "            # sns.distplot(number_of_communities, hist=False, ax=ax[1], label=label, color=palette[j])\n",
    "            # sns.distplot(community_sizes, hist=False, ax=ax[2], label=label, color=palette[j])\n",
    "\n",
    "\n",
    "\n",
    "            j += 1\n",
    "\n",
    "            for r, k_prime in enumerate(sorted(louvain_modularity[method].keys())):\n",
    "                if k_prime[-1] < k[-1]:\n",
    "                    \n",
    "                    modularities_prime = np.array(louvain_modularity[method][k_prime])\n",
    "                    number_of_communities_prime = np.array([len(c) for c in louvain_communities[method][k_prime]])\n",
    "                    community_sizes_prime = np.array([len(v) for c in louvain_communities[method][k_prime] for v in c])\n",
    "\n",
    "                    print(f'T-test Temperatures: {k[-1]}, {k_prime[-1]} Modularity: {stats.ttest_ind(modularities, modularities_prime, equal_var=False, alternative=\"greater\")}')\n",
    "                    print(f'T-test Temperatures: {k[-1]}, {k_prime[-1]} Number of Communities: {stats.ttest_ind(number_of_communities, number_of_communities_prime, equal_var=False, alternative=\"greater\")}')\n",
    "                    print(f'T-test Temperatures: {k[-1]}, {k_prime[-1]} Community Sizes: {stats.ttest_ind(community_sizes, community_sizes_prime, equal_var=False, alternative=\"less\")}')\n",
    "                    \n",
    "\n",
    "    # ax[0].legend(loc='upper right')\n",
    "\n",
    "    # fig.supxlabel('Temperature', fontsize=MEDIUM_SIZE)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig.savefig('figures/principle_3/principle_3_profiles_louvain_modularity.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    assortativities_records = []\n",
    "\n",
    "    for method in ['llm', 'random']:\n",
    "        for i, k in enumerate(sorted(location_assortativities[method].keys())):\n",
    "            if method == 'llm':\n",
    "                label = f'{k[-1]}'\n",
    "            else:\n",
    "                label = method.capitalize()[:4]\n",
    "\n",
    "            for v in location_assortativities[method][k]:\n",
    "                assortativities_records.append({\n",
    "                    'Method' : label,\n",
    "                    'Location' : v,\n",
    "                })\n",
    "\n",
    "            for v in favorite_color_assortativities[method][k]:\n",
    "                assortativities_records.append({\n",
    "                    'Method' : label,\n",
    "                    'Color' : v,\n",
    "                })\n",
    "\n",
    "            for v in hobby_assortativities[method][k]:\n",
    "                assortativities_records.append({\n",
    "                    'Method' : label,\n",
    "                    'Hobby' : v,\n",
    "                })\n",
    "\n",
    "\n",
    "    df = pd.DataFrame.from_records(assortativities_records)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "\n",
    "\n",
    "    fig.suptitle('Attribute Assortativities', fontsize=MEDIUM_SIZE)\n",
    "\n",
    "    sns.barplot(data=df, x='Method', y='Location', ax=ax[0], palette=palette)\n",
    "    sns.barplot(data=df, x='Method', y='Color', ax=ax[1], palette=palette)\n",
    "    sns.barplot(data=df, x='Method', y='Hobby', ax=ax[2], palette=palette)\n",
    "    \n",
    "    for i in range(3):\n",
    "        for tick in ax[i].get_xticklabels():\n",
    "            tick.set_fontsize(SMALL_SIZE)\n",
    "\n",
    "\n",
    "        ax[i].set_xlabel('')\n",
    "        ax[i].spines[['right', 'top']].set_visible(False)\n",
    "\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig.savefig('figures/principle_3/principle_3_assortativities.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "def get_table(filenames, sfx='', attributes=['Location', 'Favorite Color', 'Hobby'], environments=True, profiles_filename='outputs/profiles.jsonl', mutual_acceptance=False, communities=True):\n",
    "    records_assortativities = []\n",
    "    records_communities = []\n",
    "\n",
    "    for filename in filenames:\n",
    "        suffix = os.path.split(os.path.splitext(filename)[0])[-1]\n",
    "        suffix = suffix.split('+')\n",
    "\n",
    "        if len(suffix) == 3:\n",
    "            model = suffix[-2]\n",
    "            environment = suffix[-1]\n",
    "        elif len(suffix) == 2:\n",
    "            model = suffix[-1]\n",
    "            environment = 'Baseline'\n",
    "        else:\n",
    "            environment = 'Baseline' \n",
    " \n",
    "        with open(filename) as f:\n",
    "            lines = f.read().splitlines()\n",
    "\n",
    "        data = []\n",
    "\n",
    "\n",
    "        for line in lines:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "        with open(profiles_filename) as f:\n",
    "            profiles = f.read().splitlines()\n",
    "            profiles = [json.loads(profile) for profile in profiles]\n",
    "\n",
    "        profiles_dict = {str(profile['name']) : profile for profile in profiles}\n",
    "\n",
    "        for d in data:\n",
    "            Gs = []\n",
    "            for graph in d['graphs']:\n",
    "                G = nx.from_dict_of_dicts(graph)\n",
    "                \n",
    "                nx.set_node_attributes(G, profiles_dict)\n",
    "\n",
    "                G.remove_edges_from(nx.selfloop_edges(G))\n",
    "                G.remove_nodes_from(list(nx.isolates(G)))\n",
    "\n",
    "                Gs.append(G)\n",
    "\n",
    "            # import pdb; pdb.set_trace()\n",
    "\n",
    "            louvain_communities = nx.algorithms.community.louvain_communities(Gs[-1], weight='similarity')\n",
    "\n",
    "            louvain_modularity = nx.algorithms.community.modularity(Gs[-1], louvain_communities, weight='similarity')\n",
    "            louvain_num_communities = len(louvain_communities)\n",
    "            louvain_community_average_size = np.mean([len(c) for c in louvain_communities])\n",
    "\n",
    "            location_assortativity = nx.attribute_assortativity_coefficient(Gs[-1], 'location')\n",
    "            favorite_color_assortativity = nx.attribute_assortativity_coefficient(Gs[-1], 'favorite color')\n",
    "            hobby_assortativity = nx.attribute_assortativity_coefficient(Gs[-1], 'hobby')\n",
    "            lucky_number_assortativity = nx.attribute_assortativity_coefficient(Gs[-1], 'lucky number')\n",
    "\n",
    "            mutual_acceptance_probability = d.get('mutual_acceptance_probability', 100)\n",
    "\n",
    "            G_random = network_growth(d['n'], d['temperature'], method='random', model='', environment='', role='')[0][-1]\n",
    "\n",
    "            nx.set_node_attributes(G_random, {int(profile['name']) : profile for profile in profiles})\n",
    "\n",
    "            location_assortativity_random = nx.attribute_assortativity_coefficient(G_random, 'location')\n",
    "            favorite_color_assortativity_random = nx.attribute_assortativity_coefficient(G_random, 'favorite color')\n",
    "            hobby_assortativity_random = nx.attribute_assortativity_coefficient(G_random, 'hobby')\n",
    "            lucky_number_assortativity_random = nx.attribute_assortativity_coefficient(G_random, 'lucky number')\n",
    "\n",
    "            \n",
    "\n",
    "            record = {\n",
    "                'Temperature' : d['temperature'],\n",
    "                'Model' : model,\n",
    "                'Environment' : environment,\n",
    "                'Location' : location_assortativity,\n",
    "                'Favorite Color' : favorite_color_assortativity,\n",
    "                'Hobby' : hobby_assortativity,\n",
    "                'Lucky Number' : lucky_number_assortativity,\n",
    "                'Modularity' : louvain_modularity,\n",
    "                'Mutual Acceptance Probability' : mutual_acceptance_probability,\n",
    "            }\n",
    "\n",
    "            record_random = {\n",
    "                'Temperature' : d['temperature'],\n",
    "                'Model' : 'Random',\n",
    "                'Environment' : 'Baseline',\n",
    "                'Location' : location_assortativity_random,\n",
    "                'Favorite Color' : favorite_color_assortativity_random,\n",
    "                'Lucky Number' : lucky_number_assortativity_random,\n",
    "                'Hobby' : hobby_assortativity_random,\n",
    "                'Mutual Acceptance Probability' : 100,\n",
    "            }\n",
    "\n",
    "            records_assortativities.append(record)\n",
    "            records_assortativities.append(record_random)\n",
    "\n",
    "            record_communities = {\n",
    "                'Temperature' : d['temperature'],\n",
    "                'Model' : model,\n",
    "                'Environment' : environment,\n",
    "                'Modularity' : louvain_modularity,\n",
    "                'Number of Communities' : louvain_num_communities,\n",
    "                'Average Community Size' : louvain_community_average_size\n",
    "            }\n",
    "\n",
    "            records_communities.append(record_communities)\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(records_assortativities)\n",
    "\n",
    "    # average over simulations\n",
    "    df_groupped = df.groupby(['Model', 'Environment', 'Temperature']).mean().reset_index()\n",
    "\n",
    "    # do t-test with random\n",
    "    for temperature in df['Temperature'].unique():\n",
    "        for metric in attributes:\n",
    "            for model in df['Model'].unique():\n",
    "                if model == 'Random':\n",
    "                    continue\n",
    "                data = df[(df['Temperature'] == temperature) & (df['Model'] == model)][metric].values\n",
    "                data_random = df[(df['Temperature'] == temperature) & (df['Model'] == 'Random')][metric].values\n",
    "\n",
    "                t, p = stats.ttest_ind(data, data_random, equal_var=False)\n",
    "\n",
    "                # Get stars\n",
    "                if p < 0.001:\n",
    "                    p = '***'\n",
    "                elif p < 0.01:\n",
    "                    p = '**'\n",
    "                elif p < 0.05:\n",
    "                    p = '*'\n",
    "                else:\n",
    "                    p = ''\n",
    "\n",
    "                if p != '':\n",
    "                    df_groupped.loc[(df_groupped['Temperature'] == temperature) & (df_groupped['Model'] == model), metric] = f'{df_groupped.loc[(df_groupped[\"Temperature\"] == temperature) & (df_groupped[\"Model\"] == model), metric].values[0]:.3f} ({p})'\n",
    "\n",
    "    df_groupped = df_groupped[df_groupped['Model'] != 'Random']\n",
    "\n",
    "    df_groupped.sort_values(['Model', 'Environment', 'Temperature'], inplace=True)\n",
    "\n",
    "    df_groupped.to_csv('tables/principle_3_assortativities.csv', index=False)\n",
    "    df_groupped.to_latex('tables/principle_3_assortativities.tex', index=False, escape=False, float_format=\"%.2f\")   \n",
    "\n",
    "    df_communities = pd.DataFrame(records_communities)\n",
    "\n",
    "    df_communities_groupped = df_communities.groupby(['Model', 'Environment', 'Temperature']).mean().reset_index()\n",
    "\n",
    "    # do t-test of Louvain modularity with 0\n",
    "    for temperature in df_communities['Temperature'].unique():\n",
    "        for model in df_communities['Model'].unique():\n",
    "            if model == 'Random':\n",
    "                continue\n",
    "            data = df_communities[(df_communities['Temperature'] == temperature) & (df_communities['Model'] == model)]['Modularity'].values\n",
    "\n",
    "            t, p = stats.ttest_1samp(data, 0, alternative='greater')\n",
    "\n",
    "            # Get stars\n",
    "            if p < 0.001:\n",
    "                p = '***'\n",
    "            elif p < 0.01:\n",
    "                p = '**'\n",
    "            elif p < 0.05:\n",
    "                p = '*'\n",
    "            else:\n",
    "                p = ''\n",
    "\n",
    "            if p != '':\n",
    "                df_communities_groupped.loc[(df_communities_groupped['Temperature'] == temperature) & (df_communities_groupped['Model'] == model), 'Modularity'] = f'{df_communities_groupped.loc[(df_communities_groupped[\"Temperature\"] == temperature) & (df_communities_groupped[\"Model\"] == model), \"Modularity\"].values[0]:.3f} ({p})'\n",
    "\n",
    "    df_communities_groupped = df_communities_groupped[df_communities_groupped['Model'] != 'Random']\n",
    "\n",
    "    df_communities_groupped.sort_values(['Model', 'Environment', 'Temperature'], inplace=True)\n",
    "\n",
    "    df_communities_groupped.to_csv('tables/principle_3_communities.csv', index=False)\n",
    "    df_communities_groupped.to_latex('tables/principle_3_communities.tex', index=False, escape=False, float_format=\"%.2f\")\n",
    "\n",
    "    rename_models = {\n",
    "        'gpt-3.5-turbo' : 'GPT-3.5',\n",
    "        'gpt-4o-mini' : 'GPT-4 Mini',\n",
    "        'meta-meta-llama-3-70b-instruct' : 'LLAMA-3',\n",
    "        'claude-3-5-sonnet-20240620' : 'Claude 3.5',\n",
    "        'gpt-3.5-turbo_cot' : 'GPT-3.5',\n",
    "        'gpt-4o-mini_cot' : 'GPT-4 Mini',\n",
    "        'meta-meta-llama-3-70b-instruct_cot' : 'LLAMA-3',\n",
    "        'claude-3-5-sonnet-20240620_cot' : 'Claude 3.5',\n",
    "    }\n",
    "\n",
    "    rename_env = {\n",
    "        'school' : 'Sch',\n",
    "        'work' : 'Wrk',\n",
    "        'community' : 'Cmn',\n",
    "        'school_cot' : 'Sch',\n",
    "        'work_cot' : 'Wrk',\n",
    "        'community_cot' : 'Cmn',\n",
    "    }\n",
    "\n",
    "    df['Model'] = df['Model'].apply(lambda x: rename_models.get(x, x))\n",
    "\n",
    "    df_baseline = df[df['Environment'] == 'Baseline']\n",
    "    df_baseline = df_baseline[df_baseline['Model'] != 'Random']\n",
    "\n",
    "\n",
    "    df_nonbaseline = df[df['Environment'] != 'Baseline']\n",
    "    df_nonbaseline['Environment'] = df_nonbaseline['Environment'].apply(lambda x: rename_env.get(x, x))\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(1 + int(environments), len(attributes) + int(mutual_acceptance) + int(communities), figsize=(5 * (len(attributes) + int(mutual_acceptance) + int(communities)), 5 * (1 + int(environments))), squeeze=False)\n",
    "\n",
    "    \n",
    "    # df_random = df[df['Model'] == 'Random']\n",
    "    \n",
    "    for i, y in enumerate(attributes):\n",
    "        print('attribute', y)\n",
    "        sns.barplot(data=df_baseline, y=y, x=\"Temperature\", hue=\"Model\", palette=['#e67e22', '#f1c40f', '#3498db', '#7f8c8d', '#c0392b', '#34495e', '#2980b9'], ax=ax[0, i], orient='v')\n",
    "\n",
    "        if environments:\n",
    "            sns.barplot(data=df_nonbaseline, y=y, x=\"Environment\", color='#2980b9', ax=ax[1, i], orient='v')\n",
    "\n",
    "    df_communities_baseline = df_communities[df_communities['Environment'] == 'Baseline']\n",
    "    df_communities_baseline = df_communities_baseline[df_communities_baseline['Model'] != 'Random']\n",
    "    df_communities_baseline['Model'] = df_communities_baseline['Model'].apply(lambda x: rename_models.get(x, x))\n",
    "\n",
    "    if communities:\n",
    "        sns.barplot(data=df_communities_baseline, y='Modularity', x='Temperature', hue='Model', palette=['#e67e22', '#f1c40f', '#3498db', '#7f8c8d', '#c0392b', '#34495e', '#2980b9'], ax=ax[0, len(attributes)])\n",
    "        ax[0, len(attributes)].set_ylabel('Modularity')\n",
    "        ax[0 ,len(attributes)].set_title('')\n",
    "\n",
    "    if environments:\n",
    "        df_communities_nonbaseline = df_communities[df_communities['Environment'] != 'Baseline']\n",
    "        df_communities_nonbaseline['Environment'] = df_communities_nonbaseline['Environment'].apply(lambda x: rename_env.get(x, x))\n",
    "        df_communities_nonbaseline['Model'] = df_communities_nonbaseline['Model'].apply(lambda x: rename_models.get(x, x))\n",
    "\n",
    "\n",
    "        if communities:\n",
    "            sns.barplot(data=df_communities_nonbaseline, y='Modularity', x='Environment', color='#2980b9', ax=ax[1, len(attributes)])\n",
    "            ax[1, len(attributes)].set_ylabel('Modularity')\n",
    "            ax[1, len(attributes)].set_title('')\n",
    "\n",
    "    if mutual_acceptance:\n",
    "        sns.barplot(data=df_baseline, y='Mutual Acceptance Probability', x='Temperature', hue='Model', palette=['#e67e22', '#f1c40f', '#3498db', '#7f8c8d', '#c0392b', '#34495e', '#2980b9'], ax=ax[0, len(attributes) + int(communities)])\n",
    "\n",
    "        if environments:\n",
    "            sns.barplot(data=df_nonbaseline, y='Mutual Acceptance Probability', x='Environment', color='#2980b9', ax=ax[1, len(attributes) + int(communities)])\n",
    "\n",
    "    # put legend of ax[0, 0] on top of figure\n",
    "    # ax[0, 0].legend(loc='upper right', bbox_to_anchor=(1, 1))\n",
    "\n",
    "    for i in range(int(communities) + len(attributes) + int(mutual_acceptance)):\n",
    "        for j in range(1 + int(environments)):\n",
    "\n",
    "            ax[j, i].set_ylabel('')\n",
    "            ax[j, i].set_xlabel('')\n",
    "            ax[j, i].spines[['right', 'top']].set_visible(False)\n",
    "            ax[j, i].set_ylim(0, 1)\n",
    "            # ax[j, i].axhline(y=0, color='black', linestyle='--')\n",
    "\n",
    "    for i, attribute in enumerate(attributes):\n",
    "        ax[0, i].set_title(attribute)\n",
    "\n",
    "\n",
    "    for i in range(1 + len(attributes) + int(mutual_acceptance)):\n",
    "        for j in range(1 + int(environments)):\n",
    "            ax[j, i].legend().set_visible(False)\n",
    "\n",
    "    # use one of the legends of top row to create a legend for the whole figure\n",
    "    handles, labels = ax[0, 0].get_legend_handles_labels()\n",
    "\n",
    "    ax[0, 0].set_ylabel('Assortativity')\n",
    "    if environments:\n",
    "        ax[1, 0].set_ylabel('Assortativity')\n",
    "\n",
    "\n",
    "    if communities:\n",
    "        ax[0, len(attributes)].set_ylabel('Modularity')\n",
    "        if environments:\n",
    "            ax[1, len(attributes)].set_ylabel('Modularity')\n",
    "\n",
    "    if mutual_acceptance:\n",
    "        ax[0, len(attributes) + int(communities)].set_ylabel('Mutual Acceptance Prob')\n",
    "\n",
    "\n",
    "    # put figure legend on top of figure above titles\n",
    "    fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.1), ncol=4)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig.savefig(f'figures/assortativities{sfx}.png', dpi=300, bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_outfiles = []\n",
    "\n",
    "for environment, role in zip(['school', 'work', 'community'], ['classmates', 'colleagues', 'neighbors']):\n",
    "    outfile = f'outputs/principle_3_neighbors+gpt-3.5-turbo+{environment}.jsonl'\n",
    "    run_network_formation_experiment(50, 50, 1, 10, outfile, [0.5], environment=environment, role=role, method='llm', model='gpt-3.5-turbo')\n",
    "\n",
    "    table_outfiles.append(outfile)\n",
    "\n",
    "for model in ['gpt-3.5-turbo', 'meta/meta-llama-3-70b-instruct', 'gpt-4o-mini', 'claude-3-5-sonnet-20240620']:\n",
    "    outfile = f'outputs/principle_3_neighbors+{model.replace(\"/\", \"-\")}.jsonl'\n",
    "    if model == 'claude-3-5-sonnet-20240620':\n",
    "        run_network_formation_experiment(50, 50, 1, 5, outfile, [0.5, 1.0], environment=None, role='friends', method='llm', model=model)\n",
    "    else:\n",
    "        run_network_formation_experiment(50, 50, 1, 5, outfile, [0.5, 1.0, 1.5], environment=None, role='friends', method='llm', model=model)\n",
    "\n",
    "    table_outfiles.append(outfile)\n",
    "\n",
    "get_table(table_outfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_outfiles_no_color = []\n",
    "table_outfiles_lucky_number = []\n",
    "\n",
    "for model in ['gpt-3.5-turbo', 'meta/meta-llama-3-70b-instruct', 'gpt-4o-mini', 'claude-3-5-sonnet-20240620']:\n",
    "   \n",
    "    outfile = f'outputs/principle_3_neighbors_lucky_number+{model.replace(\"/\", \"-\")}.jsonl'\n",
    "\n",
    "    if model == 'claude-3-5-sonnet-20240620':\n",
    "        run_network_formation_experiment(50, 50, 1, 5, outfile, [0.5, 1.0], environment=None, role='friends', method='llm', model=model, profiles_filename='outputs/profiles_with_lucky_number.jsonl')\n",
    "    else:\n",
    "        run_network_formation_experiment(50, 50, 1, 5, outfile, [0.5, 1.0, 1.5], environment=None, role='friends', method='llm', model=model, profiles_filename='outputs/profiles_with_lucky_number.jsonl')\n",
    "\n",
    "    table_outfiles_lucky_number.append(outfile)\n",
    "\n",
    "\n",
    "get_table(table_outfiles_no_color, attributes=['Location', 'Hobby'], environments=False, sfx='_no_color', profiles_filename='outputs/profiles_without_color.jsonl')\n",
    "get_table(table_outfiles_lucky_number, attributes=['Location', 'Hobby', 'Favorite Color', 'Lucky Number'], environments=False, sfx='_lucky_number', profiles_filename='outputs/profiles_with_lucky_number.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_outfiles_mutual_acceptance = []\n",
    "\n",
    "for model in ['gpt-3.5-turbo', 'gpt-4o-mini', 'meta/meta-llama-3-70b-instruct']:\n",
    "    outfile = f'outputs/principle_3_neighbors_mutual_acceptance+{model.replace(\"/\", \"-\")}.jsonl'\n",
    "    run_network_formation_experiment(50, 50, 1, 5, outfile, [0.5], environment=None, role='friends', method='llm', model=model, mutual_acceptance=True)\n",
    "\n",
    "    table_outfiles_mutual_acceptance.append(outfile)\n",
    "\n",
    "\n",
    "get_table(table_outfiles_mutual_acceptance, attributes=['Location', 'Hobby', 'Favorite Color'], environments=False, sfx='_mutual_acceptance', mutual_acceptance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_outfiles_cot = []\n",
    "\n",
    "for environment, role in zip(['school', 'work', 'community'], ['classmates', 'colleagues', 'neighbors']):\n",
    "    outfile = f'outputs/principle_3_neighbors+gpt-3.5-turbo+{environment}_cot.jsonl'\n",
    "    run_network_formation_experiment(50, 50, 1, 5, outfile, [0.5, 1.0, 1.5], environment=environment, role=role, method='llm', model='gpt-3.5-turbo', cot=True)\n",
    "    table_outfiles_cot.append(outfile)\n",
    "\n",
    "for model in ['gpt-3.5-turbo', 'meta/meta-llama-3-70b-instruct', 'gpt-4o-mini', 'claude-3-5-sonnet-20240620']:\n",
    "    outfile = f'outputs/principle_3_neighbors+{model.replace(\"/\", \"-\")}_cot.jsonl'\n",
    "\n",
    "    if model == 'claude-3-5-sonnet-20240620':\n",
    "        run_network_formation_experiment(50, 50, 1, 5, outfile, [0.5, 1.0], environment=None, role='friends', method='llm', model=model, cot=True)\n",
    "    else:\n",
    "        run_network_formation_experiment(50, 50, 1, 5, outfile, [0.5, 1.0, 1.5], environment=None, role='friends', method='llm', model=model, cot=True)\n",
    "    \n",
    "    table_outfiles_cot.append(outfile)\n",
    "\n",
    "get_table(table_outfiles_cot, sfx=\"_cot\", environments=True, mutual_acceptance=False, communities=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
